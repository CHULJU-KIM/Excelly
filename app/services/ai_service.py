# app/services/ai_service.py
# AI model management service

import time
import asyncio
import json
from typing import Optional, Dict, Any, List, Tuple
from openai import AsyncOpenAI
import google.generativeai as genai
from PIL import Image
import io

from app.core.config import settings
from app.core.exceptions import AIServiceException
from app.models.chat import QuestionClassification, UserIntent, AIResponse, ConversationState
from prompts import PLANNING_PERSONA_PROMPT, CODING_PERSONA_PROMPT, DEBUGGING_PERSONA_PROMPT, PYTHON_PERSONA_PROMPT

class AIService:
    """Service for managing AI model interactions with intelligent routing"""
    
    def __init__(self):
        self.openai_client = None
        self.gemini_pro_model = None  # 2.5 Pro - ìµœìƒìœ„ ë‚œì´ë„
        self.gemini_2_5_flash_model = None  # 2.5 Flash - ì¤‘ê°„ ë‚œì´ë„
        self.gemini_2_0_flash_model = None  # 2.0 Flash - ì¼ë°˜ ë‚œì´ë„
        self._initialize_clients()
    
    def _initialize_clients(self):
        """Initialize AI clients with difficulty-based model selection"""
        try:
            # Initialize OpenAI client (auxiliary role)
            if settings.OPENAI_API_KEY:
                self.openai_client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
                print(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ: {settings.OPENAI_MODEL}")
            
            # Initialize Gemini client
            if settings.GEMINI_API_KEY:
                genai.configure(api_key=settings.GEMINI_API_KEY)
                
                # Initialize Gemini 2.5 Pro (highest difficulty - creative, innovative)
                try:
                    self.gemini_pro_model = genai.GenerativeModel(settings.GEMINI_PRO_MODEL)
                    print(f"âœ… Gemini 2.5 Pro ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ: {settings.GEMINI_PRO_MODEL}")
                except Exception as e:
                    print(f"âš ï¸ Gemini 2.5 Pro ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    try:
                        self.gemini_pro_model = genai.GenerativeModel(settings.GEMINI_2_0_PRO_FALLBACK)
                        print(f"âœ… Gemini 2.0 Pro ëª¨ë¸ë¡œ ëŒ€ì²´: {settings.GEMINI_2_0_PRO_FALLBACK}")
                    except Exception as e2:
                        print(f"âŒ Gemini Pro ê³„ì—´ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e2}")
                        self.gemini_pro_model = None
                
                # Initialize Gemini 2.5 Flash (medium difficulty - complex analysis, planning)
                try:
                    self.gemini_2_5_flash_model = genai.GenerativeModel(settings.GEMINI_2_5_FLASH_MODEL)
                    print(f"âœ… Gemini 2.5 Flash ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ: {settings.GEMINI_2_5_FLASH_MODEL}")
                except Exception as e:
                    print(f"âš ï¸ Gemini 2.5 Flash ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    try:
                        self.gemini_2_5_flash_model = genai.GenerativeModel(settings.GEMINI_1_5_FLASH_FALLBACK)
                        print(f"âœ… Gemini 1.5 Flash ëª¨ë¸ë¡œ ëŒ€ì²´: {settings.GEMINI_1_5_FLASH_FALLBACK}")
                    except Exception as e2:
                        print(f"âŒ Gemini 2.5 Flash ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e2}")
                        self.gemini_2_5_flash_model = None
                
                # Initialize Gemini 2.0 Flash (general difficulty - simple questions, classification)
                try:
                    self.gemini_2_0_flash_model = genai.GenerativeModel(settings.GEMINI_2_0_FLASH_MODEL)
                    print(f"âœ… Gemini 2.0 Flash ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ: {settings.GEMINI_2_0_FLASH_MODEL}")
                except Exception as e:
                    print(f"âš ï¸ Gemini 2.0 Flash ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    try:
                        self.gemini_2_0_flash_model = genai.GenerativeModel(settings.GEMINI_1_5_FLASH_FALLBACK)
                        print(f"âœ… Gemini 1.5 Flash ëª¨ë¸ë¡œ ëŒ€ì²´: {settings.GEMINI_1_5_FLASH_FALLBACK}")
                    except Exception as e2:
                        print(f"âŒ Gemini 2.0 Flash ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e2}")
                        self.gemini_2_0_flash_model = None
                
        except Exception as e:
            print(f"âš ï¸ AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # Don't raise exception, continue with available models
    
    async def classify_question(self, question: str, context: str = "") -> QuestionClassification:
        """User-level aware question classification with problem recognition"""
        try:
            # ì‚¬ìš©ì ìˆ˜ì¤€ ë° ë¬¸ì œ ìœ í˜• ê°ì§€
            question_lower = question.lower().strip()
            
            # ì´ˆë³´ì í‘œí˜„ í‚¤ì›Œë“œ ê°ì§€
            beginner_keywords = [
                "ëª¨ë¥´ê² ", "ëª¨ë¥´ëŠ”", "ëª¨ë¦„", "ì²˜ìŒ", "ì´ˆë³´", "ì–´ë–»ê²Œ", "ë°©ë²•", "ë„ì™€ì¤˜", "ë„ì™€ì£¼ì„¸ìš”",
                "ì•ˆë˜", "ì•ˆë¼", "ì˜¤ë¥˜", "ì—ëŸ¬", "ë¬¸ì œ", "í‹€ë ¸", "ì˜ëª»", "ì´ìƒí•´", "ì™œ",
                "ë­ê°€", "ì–´ë””ì„œ", "ì–´ë–¤", "ë¬´ì—‡", "ì–¸ì œ", "ì–´ëŠ", "ë­˜", "ë­”ì§€"
            ]
            
            # ê³ ê¸‰ ì‚¬ìš©ì í‘œí˜„ í‚¤ì›Œë“œ ê°ì§€  
            advanced_keywords = [
                "vlookup", "index", "match", "pivot", "ë§¤í¬ë¡œ", "vba", "í•¨ìˆ˜ì¡°í•©", "ë°°ì—´ìˆ˜ì‹",
                "ë™ì ", "ìë™í™”", "ìµœì í™”", "ì•Œê³ ë¦¬ì¦˜", "ì •ê·œì‹", "api", "sql"
            ]
            
            # íŒŒì´ì¬ ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€ (íŒŒì´ì¬ ì†”ë£¨ì…˜ì„ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•˜ëŠ” ê²½ìš°)
            python_keywords = [
                "íŒŒì´ì¬", "python", "pandas", "numpy", "matplotlib", "openpyxl", "xlrd",
                "ìŠ¤í¬ë¦½íŠ¸", "í”„ë¡œê·¸ë¨", "ì½”ë”©", "ê°œë°œ", "ë¼ì´ë¸ŒëŸ¬ë¦¬", "ëª¨ë“ˆ", "import",
                "ë°ì´í„°í”„ë ˆì„", "dataframe", "ì‹œë¦¬ì¦ˆ", "series", "forë¬¸", "whileë¬¸",
                "í•¨ìˆ˜ì •ì˜", "def", "í´ë˜ìŠ¤", "class", "ê°ì²´", "object"
            ]
            
            # ë³µì¡í•œ ì‘ì—… í‚¤ì›Œë“œ ê°ì§€ (ê³ ê¸‰ ì½”ë”©/ë³µì¡í•œ ë¡œì§)
            complex_keywords = [
                "ë³µì¡í•œ", "ê³ ê¸‰", "ì •êµí•œ", "ìµœì í™”", "ì„±ëŠ¥", "ëŒ€ìš©ëŸ‰", "í†µí•©", "ì—°ë™",
                "ìë™í™”", "ë§¤í¬ë¡œ", "vba", "ìŠ¤í¬ë¦½íŠ¸", "í”„ë¡œê·¸ë¨", "í•¨ìˆ˜ì¡°í•©", "ë°°ì—´ìˆ˜ì‹",
                "ë™ì ", "ì‹¤ì‹œê°„", "ë‹¤ì¤‘", "ë³‘ë ¬", "ë¹„ë™ê¸°", "ì´ë²¤íŠ¸", "ì½œë°±"
            ]
            
            # ëŒ€í™” ì—°ê²°ì„± í‚¤ì›Œë“œ ê°ì§€
            continuation_keywords = [
                "ê³„ì†", "ê³„ì†í•´", "ì´ì–´ì„œ", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ì¶”ê°€ë¡œ", "ë”", "ë‹¤ìŒ",
                "ìœ„ì—ì„œ", "ì•ì—ì„œ", "ì´ì „", "ë°©ê¸ˆ", "ì•„ê¹Œ", "ê·¸ê±°", "ê·¸ê²ƒ", "ì´ê²ƒ"
            ]
            
            # ì‚¬ìš©ì ìˆ˜ì¤€ íŒë‹¨
            is_beginner = any(keyword in question_lower for keyword in beginner_keywords)
            is_advanced = any(keyword in question_lower for keyword in advanced_keywords)
            is_complex = any(keyword in question_lower for keyword in complex_keywords)
            is_continuation = any(keyword in question for keyword in continuation_keywords)
            is_python_request = any(keyword in question_lower for keyword in python_keywords)
            has_context = bool(context and context.strip())
            
            # ë¬¸ì œ ìœ í˜• ë¶„ì„
            problem_indicators = {
                "formula_error": ["#n/a", "#value", "#ref", "#div/0", "#name", "ì˜¤ë¥˜", "ì—ëŸ¬"],
                "data_issue": ["ì¤‘ë³µ", "ë¹ˆì¹¸", "ê³µë°±", "ëˆ„ë½", "ì •ë ¬", "í•„í„°"],
                "calculation": ["í•©ê³„", "í‰ê· ", "ê°œìˆ˜", "ìµœëŒ€", "ìµœì†Œ", "ê³„ì‚°"],
                "lookup": ["ì°¾ê¸°", "ê²€ìƒ‰", "ì¡°íšŒ", "ë§¤ì¹­", "ì¼ì¹˜"],
                "formatting": ["ì„œì‹", "ìƒ‰ê¹”", "êµµê²Œ", "ì •ë ¬", "í¬ê¸°"],
                "automation": ["ìë™", "ë°˜ë³µ", "ì¼ê´„", "í•œë²ˆì—", "ë§¤í¬ë¡œ"]
            }
            
            detected_problems = []
            for problem_type, indicators in problem_indicators.items():
                if any(indicator in question_lower for indicator in indicators):
                    detected_problems.append(problem_type)
            
            # ë¶„ë¥˜ ë¡œì§ ê°œì„  (íŒŒì´ì¬ ìš”ì²­ ìš°ì„  ì²˜ë¦¬)
            if is_python_request:
                classification = "python_coding"  # íŒŒì´ì¬ ì „ìš© ë¶„ë¥˜
                confidence = 0.95
                recommended_model = "gemini_pro"
            elif is_continuation and has_context:
                classification = "continuation"
                confidence = 0.9
                recommended_model = "gemini_2_5_flash"
            elif is_complex or (is_advanced and len(detected_problems) > 2):
                classification = "advanced_coding"  # ìƒˆë¡œìš´ ë¶„ë¥˜
                confidence = 0.9
                recommended_model = "gemini_pro"
            elif is_beginner or not detected_problems:
                classification = "beginner_help"
                confidence = 0.8
                recommended_model = "gemini_2_0_flash"
            elif "automation" in detected_problems or is_advanced:
                classification = "coding"
                confidence = 0.8
                recommended_model = "gemini_2_5_flash"
            elif len(detected_problems) > 2:
                classification = "hybrid"
                confidence = 0.7
                recommended_model = "gemini_2_5_flash"
            elif "lookup" in detected_problems or "calculation" in detected_problems:
                classification = "analysis"
                confidence = 0.7
                recommended_model = "gemini_2_5_flash"
            else:
                classification = "simple"
                confidence = 0.6
                recommended_model = "gemini_2_0_flash"
            
            return QuestionClassification(
                classification=classification,
                confidence=confidence,
                reasoning=f"ì‚¬ìš©ì ìˆ˜ì¤€: {'ì´ˆë³´' if is_beginner else 'ê³ ê¸‰' if is_advanced else 'ì¼ë°˜'}, ë³µì¡ë„: {'ë³µì¡' if is_complex else 'ì¼ë°˜'}, ê°ì§€ëœ ë¬¸ì œ: {detected_problems}",
                recommended_model=recommended_model,
                estimated_tokens=500,
                needs_clarification=False,
                clarification_reasons=[]
            )
            
        except Exception as e:
            return QuestionClassification(
                classification="beginner_help",
                confidence=0.5,
                reasoning=f"ë¶„ë¥˜ ì‹¤íŒ¨, ì´ˆë³´ì ëª¨ë“œë¡œ ì²˜ë¦¬: {str(e)}",
                recommended_model="gemini_2_0_flash",
                estimated_tokens=500
            )
    
    def _fallback_classification(self, response: str) -> str:
        """Fallback classification when JSON parsing fails"""
        response_lower = response.lower()
        if any(word in response_lower for word in ["beginner", "ì´ˆë³´", "ëª¨ë¥´", "ë„ì™€", "ì–´ë–»ê²Œ"]):
            return "beginner_help"
        elif any(word in response_lower for word in ["coding", "vba", "ë§¤í¬ë¡œ", "ì½”ë“œ", "í•¨ìˆ˜"]):
            return "coding"
        elif any(word in response_lower for word in ["analysis", "ë¶„ì„", "í†µê³„", "ë°ì´í„°"]):
            return "analysis"
        elif any(word in response_lower for word in ["planning", "ê³„íš", "êµ¬ì¡°", "ì„¤ê³„"]):
            return "planning"
        elif any(word in response_lower for word in ["simple", "ê°„ë‹¨", "ê¸°ë³¸"]):
            return "simple"
        elif any(word in response_lower for word in ["hybrid", "ë³µí•©", "ì¡°í•©"]):
            return "hybrid"
        elif any(word in response_lower for word in ["continuation", "ì—°ê²°", "ì´ì–´", "ê³„ì†"]):
            return "continuation"
        elif any(word in response_lower for word in ["debugging", "ì˜¤ë¥˜", "ë¬¸ì œ"]):
            return "debugging"
        else:
            return "beginner_help"  # Default to beginner help
    
    async def analyze_user_intent(self, plan: str, user_reply: str) -> UserIntent:
        """Analyze user's intent using Gemini 2.0 Flash (optimized for fast analysis)"""
        try:
            prompt = f"""AIì˜ ê³„íš: "{plan[:200]}..."
ì‚¬ìš©ì ë‹µë³€: "{user_reply}"

ì‚¬ìš©ì ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”:
- "agreement": ê³„íšì— ë™ì˜í•˜ê³  ì§„í–‰
- "modification": ê³„íš ìˆ˜ì • ìš”ì²­
- "clarification": ì¶”ê°€ ì„¤ëª… ìš”ì²­
- "rejection": ê³„íš ê±°ë¶€

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”:
{{
    "intent": "agreement|modification|clarification|rejection",
    "confidence": 0.0-1.0,
    "reasoning": "ì˜ë„ ë¶„ì„ ì´ìœ "
}}"""
            
            # Use Gemini 2.0 Flash for intent analysis (fastest and most cost-effective)
            if self.gemini_2_0_flash_model:
                response = await self._call_gemini_2_0_flash(prompt, temperature=0.0)
            else:
                response = await self._call_openai(prompt, model="gpt-4o-mini", temperature=0.0)
            
            try:
                result = json.loads(response)
                return UserIntent(
                    intent=result.get("intent", "other"),
                    confidence=result.get("confidence", 0.5),
                    reasoning=result.get("reasoning", "")
                )
            except json.JSONDecodeError:
                intent = self._fallback_intent_analysis(response)
                return UserIntent(
                    intent=intent,
                    confidence=0.7,
                    reasoning="JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ë¶„ì„ ì‚¬ìš©"
                )
                
        except Exception as e:
            return UserIntent(
                intent="other",
                confidence=0.5,
                reasoning=f"ì˜ë„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            )
    
    def _fallback_intent_analysis(self, response: str) -> str:
        """Fallback intent analysis when JSON parsing fails"""
        response_lower = response.lower()
        if any(word in response_lower for word in ["agreement", "ë™ì˜", "ì¢‹ë‹¤", "ì§„í–‰"]):
            return "agreement"
        elif any(word in response_lower for word in ["modification", "ìˆ˜ì •", "ë³€ê²½"]):
            return "modification"
        elif any(word in response_lower for word in ["clarification", "ì„¤ëª…", "ì´í•´"]):
            return "clarification"
        elif any(word in response_lower for word in ["rejection", "ê±°ë¶€", "ì•„ë‹ˆ"]):
            return "rejection"
        else:
            return "other"
    

    
    async def generate_simple_response(self, question: str, answer_style: Optional[str] = None) -> str:
        """Generate simple response using Gemini 2.0 Flash (cost-effective for basic queries)"""
        try:
            prompt = f"""Excel ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”: {question}

**ì¶œë ¥ í˜•ì‹**:
ğŸ“ **í•µì‹¬ ì„¤ëª…** (2-3ì¤„)
ğŸ’¡ **êµ¬ì²´ì  ì˜ˆì‹œ** (í•„ìš”ì‹œ)
âš¡ **ì¶”ê°€ íŒ** (1ì¤„)"""
            
            if answer_style == "concise":
                prompt += "\n\n[ìŠ¤íƒ€ì¼] í•µì‹¬ë§Œ 3ì¤„ ì´ë‚´"
            
            # Use Gemini 2.0 Flash for basic queries (fastest and most cost-effective)
            if self.gemini_2_0_flash_model:
                print("ğŸ¤– ê¸°ë³¸ ì§ˆì˜ ì²˜ë¦¬: Gemini 2.0 Flash ì‚¬ìš©")
                return await self._call_gemini_2_0_flash(prompt, temperature=0.3)
            elif self.gemini_2_5_flash_model:
                print("ğŸ¤– ê¸°ë³¸ ì§ˆì˜ ì²˜ë¦¬: Gemini 2.5 Flash í´ë°±")
                return await self._call_gemini_2_5_flash(prompt, temperature=0.3)
            else:
                print("ğŸ¤– ê¸°ë³¸ ì§ˆì˜ ì²˜ë¦¬: OpenAI í´ë°±")
                return await self._call_openai(prompt, model="gpt-4o-mini", temperature=0.3)
            
        except Exception as e:
            raise AIServiceException(f"ê°„ë‹¨í•œ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_coding_response(self, question: str, context: str, file_summary: str, answer_style: Optional[str] = None) -> str:
        """Generate coding response using Gemini 2.5 Flash (optimized for basic formulas and coding)"""
        try:
            prompt = f"""Excel ì½”ë”© ì‘ì—…ì„ í•´ê²°í•´ì£¼ì„¸ìš”.

**ì§ˆë¬¸**: {question}
**íŒŒì¼ ì •ë³´**: {file_summary}

**ì¶œë ¥ í˜•ì‹**:
ğŸ”§ **ì½”ë“œ êµ¬ì¡°** (VBA/í•¨ìˆ˜ ì„¤ê³„)
ğŸ“‹ **êµ¬í˜„ ì½”ë“œ** (ì™„ì „í•œ ì½”ë“œ)
ğŸ“– **ì‚¬ìš© ë°©ë²•** (ë‹¨ê³„ë³„ ì„¤ëª…)
âš ï¸ **ì£¼ì˜ì‚¬í•­** (ì½”ë“œ ì‹¤í–‰ ì‹œ ì£¼ì˜ì )
ğŸš€ **ìµœì í™” íŒ** (ì„±ëŠ¥ ê°œì„  ë°©ì•ˆ)"""
            
            if answer_style == "concise":
                prompt += "\n\n[ìŠ¤íƒ€ì¼] í•µì‹¬ë§Œ 5ì¤„ ì´ë‚´"
            
            # Use Gemini 2.5 Flash for basic coding (fast and accurate)
            if self.gemini_2_5_flash_model:
                print("ğŸ”§ ê¸°ë³¸ ìˆ˜ì‹/ì½”ë”© ì²˜ë¦¬: Gemini 2.5 Flash ì‚¬ìš©")
                return await self._call_gemini_2_5_flash(prompt, temperature=0.3)
            elif self.gemini_pro_model:
                print("ğŸ”§ ê¸°ë³¸ ìˆ˜ì‹/ì½”ë”© ì²˜ë¦¬: Gemini 2.5 Pro í´ë°±")
                return await self._call_gemini_pro(prompt, temperature=0.3)
            else:
                print("ğŸ”§ ê¸°ë³¸ ìˆ˜ì‹/ì½”ë”© ì²˜ë¦¬: OpenAI í´ë°±")
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.3)
            
        except Exception as e:
            raise AIServiceException(f"ì½”ë”© ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_advanced_coding_response(self, question: str, context: str, file_summary: str, answer_style: Optional[str] = None) -> str:
        """Generate advanced coding response using Gemini 2.5 Pro (for complex VBA and advanced functions)"""
        try:
            prompt = f"""ê³ ê¸‰ Excel ì½”ë”© ì‘ì—…ì„ í•´ê²°í•´ì£¼ì„¸ìš”.

**ì§ˆë¬¸**: {question}
**íŒŒì¼ ì •ë³´**: {file_summary}

**ì¶œë ¥ í˜•ì‹**:
ğŸ¯ **ê³ ê¸‰ ì ‘ê·¼ë²•** (ë³µì¡í•œ ë¡œì§ ì„¤ê³„)
ğŸ”§ **ì •êµí•œ ì½”ë“œ** (ìµœì í™”ëœ VBA/í•¨ìˆ˜)
ğŸ“– **ìƒì„¸í•œ ê°€ì´ë“œ** (ë‹¨ê³„ë³„ ì„¤ëª…)
âš ï¸ **ê³ ê¸‰ ì£¼ì˜ì‚¬í•­** (ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ ê³ ë ¤)
ğŸš€ **ì„±ëŠ¥ ìµœì í™”** (ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬)
ğŸ’¡ **í™•ì¥ì„± ê³ ë ¤** (ì¬ì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ì¡°)"""
            
            if answer_style == "concise":
                prompt += "\n\n[ìŠ¤íƒ€ì¼] í•µì‹¬ë§Œ 5ì¤„ ì´ë‚´"
            
            # Use Gemini 2.5 Pro for advanced coding (highest quality)
            if self.gemini_pro_model:
                print("ğŸš€ ê³ ê¸‰/ë³µì¡í•œ ì‘ì—… ì²˜ë¦¬: Gemini 2.5 Pro ì‚¬ìš©")
                return await self._call_gemini_pro(prompt, temperature=0.3)
            elif self.gemini_2_5_flash_model:
                print("ğŸš€ ê³ ê¸‰/ë³µì¡í•œ ì‘ì—… ì²˜ë¦¬: Gemini 2.5 Flash í´ë°±")
                return await self._call_gemini_2_5_flash(prompt, temperature=0.3)
            else:
                print("ğŸš€ ê³ ê¸‰/ë³µì¡í•œ ì‘ì—… ì²˜ë¦¬: OpenAI í´ë°±")
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.3)
            
        except Exception as e:
            raise AIServiceException(f"ê³ ê¸‰ ì½”ë”© ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_analysis_response(self, question: str, context: str, file_summary: str, answer_style: Optional[str] = None) -> str:
        """Generate analysis response using Gemini 2.5 Flash (optimized for data analysis)"""
        try:
            prompt = f"""Excel ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.

**ì§ˆë¬¸**: {question}
**íŒŒì¼ ì •ë³´**: {file_summary}

**ì¶œë ¥ í˜•ì‹**:
ğŸ” **ë¶„ì„ ë°©ë²•** (ì ‘ê·¼ë²• ì„¤ëª…)
ğŸ“Š **ë¶„ì„ ê²°ê³¼** (êµ¬ì²´ì  ê²°ê³¼)
ğŸ“ˆ **ì‹œê°í™” ì œì•ˆ** (ì°¨íŠ¸/ê·¸ë˜í”„)
ğŸ’¡ **ì¸ì‚¬ì´íŠ¸** (ë°œê²¬ëœ íŒ¨í„´)
ğŸ¯ **ì¶”ì²œì‚¬í•­** (ë‹¤ìŒ ë‹¨ê³„)"""
            
            if answer_style == "concise":
                prompt += "\n\n[ìŠ¤íƒ€ì¼] í•µì‹¬ë§Œ 5ì¤„ ì´ë‚´"
            
            # Use Gemini 2.5 Flash for analysis (fast and accurate)
            if self.gemini_2_5_flash_model:
                return await self._call_gemini_2_5_flash(prompt, temperature=0.5)
            elif self.gemini_2_0_flash_model:
                return await self._call_gemini_2_0_flash(prompt, temperature=0.5)
            else:
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.5)
            
        except Exception as e:
            raise AIServiceException(f"ë¶„ì„ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_planning_response(self, context: str, file_summary: str = "", answer_style: Optional[str] = None) -> str:
        """Generate planning response using Gemini 2.5 Flash (structured thinking)"""
        try:
            style_guard = "\n\n[ì‘ë‹µ ìŠ¤íƒ€ì¼] í•µì‹¬ë§Œ ê°„ê²°íˆ 5ì¤„ ì´ë‚´ë¡œ ìš”ì•½" if (answer_style=="concise") else ""
            prompt = f"{PLANNING_PERSONA_PROMPT}{style_guard}\n\n--- Conversation History ---\n{context}\n\n{file_summary}"
            
            # Use Gemini 2.5 Flash for planning (structured thinking)
            if self.gemini_2_5_flash_model:
                return await self._call_gemini_2_5_flash(prompt, temperature=0.7)
            elif self.gemini_2_0_flash_model:
                return await self._call_gemini_2_0_flash(prompt, temperature=0.7)
            else:
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.7)
            
        except Exception as e:
            raise AIServiceException(f"ê³„íš ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_hybrid_response(self, question: str, context: str, file_summary: str, answer_style: Optional[str] = None) -> str:
        """Generate hybrid response using multiple models for best quality"""
        try:
            # Step 1: Use Gemini 2.5 Pro for initial solution
            initial_prompt = f"""Excel ì‘ì—…ì˜ í•µì‹¬ í•´ê²°ì±…ì„ ì œì‹œí•´ì£¼ì„¸ìš”: {question}

íŒŒì¼ ì •ë³´: {file_summary}

**ì¶œë ¥ í˜•ì‹**:
ğŸ¯ **í•µì‹¬ ì ‘ê·¼ë²•** (2ì¤„)
ğŸ”§ **ì£¼ìš” í•´ê²°ì±…** (3ì¤„)
ğŸ“‹ **êµ¬í˜„ ë°©ë²•** (2ì¤„)"""
            
            if self.gemini_pro_model:
                initial_response = await self._call_gemini_pro(initial_prompt, temperature=0.5)
            else:
                initial_response = await self._call_openai(initial_prompt, model=settings.OPENAI_MODEL, temperature=0.5)
            
            # Step 2: Use OpenAI for refinement and optimization
            refinement_prompt = f"""ë‹¤ìŒ Excel í•´ê²°ì±…ì„ ìµœì í™”í•˜ê³  ë³´ì™„í•´ì£¼ì„¸ìš”:

**ì›ë³¸ í•´ê²°ì±…**:
{initial_response}

**ìš”ì²­ì‚¬í•­**: {question}

**ìµœì í™” ìš”ì²­**:
1. ì½”ë“œ í’ˆì§ˆ í–¥ìƒ
2. ì˜¤ë¥˜ ì²˜ë¦¬ ì¶”ê°€
3. ì„±ëŠ¥ ìµœì í™”
4. ì‚¬ìš©ì ì¹œí™”ì  ì„¤ëª…

**ì¶œë ¥ í˜•ì‹**:
âœ… **ìµœì í™”ëœ í•´ê²°ì±…** (ê°œì„ ëœ ì½”ë“œ/ë°©ë²•)
ğŸ” **ìƒì„¸ ì„¤ëª…** (ë‹¨ê³„ë³„ ê°€ì´ë“œ)
âš ï¸ **ì£¼ì˜ì‚¬í•­** (ì‹¤í–‰ ì‹œ ì£¼ì˜ì )
ğŸ’¡ **ì¶”ê°€ íŒ** (ê³ ê¸‰ í™œìš©ë²•)"""
            
            if self.openai_client:
                refined_response = await self._call_openai(refinement_prompt, model=settings.OPENAI_MODEL, temperature=0.3)
            else:
                refined_response = initial_response
            
            # Step 3: Combine and format final response
            final_response = f"""ğŸš€ **í•˜ì´ë¸Œë¦¬ë“œ AI ìµœì í™” ì†”ë£¨ì…˜**

{refined_response}

---
*ì´ ë‹µë³€ì€ Gemini 2.5 Proì™€ OpenAIì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*"""
            
            return final_response
            
        except Exception as e:
            raise AIServiceException(f"í•˜ì´ë¸Œë¦¬ë“œ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_python_response(self, question: str, context: str, file_summary: str, answer_style: Optional[str] = None) -> str:
        """Generate Python-specific response using Python persona prompt"""
        try:
            style_guard = "\n\n[ì‘ë‹µ ìŠ¤íƒ€ì¼] í•µì‹¬ë§Œ ê°„ê²°íˆ 5ì¤„ ì´ë‚´ë¡œ ìš”ì•½" if (answer_style=="concise") else ""
            prompt = f"{PYTHON_PERSONA_PROMPT}{style_guard}\n\n--- Conversation History ---\n{context}\n\n--- File Analysis ---\n{file_summary}\n\n--- Current Question ---\n{question}"
            
            # Use Gemini Pro for Python coding (highest capability)
            if self.gemini_pro_model:
                return await self._call_gemini_pro(prompt, temperature=0.3)
            elif self.gemini_2_5_flash_model:
                return await self._call_gemini_2_5_flash(prompt, temperature=0.3)
            else:
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.3)
            
        except Exception as e:
            raise AIServiceException(f"íŒŒì´ì¬ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_continuation_response(self, question: str, context: str, file_summary: str, answer_style: Optional[str] = None) -> str:
        """Generate continuation response maintaining conversation context"""
        try:
            prompt = f"""ì´ì „ ëŒ€í™”ì™€ ì—°ê²°í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.

**í˜„ì¬ ì§ˆë¬¸**: {question}
**ì´ì „ ëŒ€í™”**: {context}
**íŒŒì¼ ì •ë³´**: {file_summary}

**ì¶œë ¥ í˜•ì‹**:
ğŸ”— **ì—°ê²°ëœ ë‚´ìš©** (ì´ì „ ëŒ€í™”ì™€ì˜ ì—°ê²°ì )
ğŸ“ **ì¶”ê°€ ì„¤ëª…** (í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€)
ğŸ¯ **êµ¬ì²´ì  ë°©ë²•** (ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„)
ğŸ’¡ **ë³´ì™„ ì‚¬í•­** (ì¶”ê°€ë¡œ ê³ ë ¤í•  ì )

ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ë©´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ì„¸ìš”."""
            
            if answer_style == "concise":
                prompt += "\n\n[ìŠ¤íƒ€ì¼] í•µì‹¬ë§Œ 5ì¤„ ì´ë‚´"
            
            # Use Gemini 2.5 Flash for continuation (context-aware processing)
            if self.gemini_2_5_flash_model:
                return await self._call_gemini_2_5_flash(prompt, temperature=0.6)
            elif self.gemini_2_0_flash_model:
                return await self._call_gemini_2_0_flash(prompt, temperature=0.6)
            else:
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.6)
            
        except Exception as e:
            raise AIServiceException(f"ì—°ê²° ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_beginner_response(self, question: str, context: str, file_summary: str, answer_style: Optional[str] = None) -> str:
        """Generate beginner-friendly response with problem understanding"""
        try:
            # ë¬¸ì œ ìƒí™©ì„ ì´í•´í•˜ê³  ì‰¬ìš´ í•´ê²°ì±… ì œì‹œ
            prompt = f"""ì´ˆë³´ìë¥¼ ìœ„í•œ Excel ë¬¸ì œ í•´ê²° ë„ìš°ë¯¸ë¡œì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**ì‚¬ìš©ì ì§ˆë¬¸**: {question}
**íŒŒì¼ ì •ë³´**: {file_summary}

**ë‹µë³€ ë°©ì‹**:
1. ë¨¼ì € ì‚¬ìš©ìê°€ ê²ªê³  ìˆëŠ” ë¬¸ì œ ìƒí™©ì„ ì •í™•íˆ íŒŒì•…í•˜ê³  ê³µê°í•´ì£¼ì„¸ìš”
2. ì „ë¬¸ ìš©ì–´ëŠ” ì‰¬ìš´ ë§ë¡œ í’€ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”  
3. ë‹¨ê³„ë³„ë¡œ ë”°ë¼í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•´ì£¼ì„¸ìš”
4. ì™œ ê·¸ë ‡ê²Œ í•´ì•¼ í•˜ëŠ”ì§€ ì´ìœ ë„ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”

**ì¶œë ¥ í˜•ì‹**:
ğŸ˜Š **ë¬¸ì œ íŒŒì•…**: (ì‚¬ìš©ì ìƒí™© ì´í•´ ë° ê³µê°)
ğŸ“ **ì‰¬ìš´ ì„¤ëª…**: (ì „ë¬¸ìš©ì–´ ì—†ì´ ì‰½ê²Œ ì„¤ëª…)
ğŸ‘† **ë”°ë¼í•˜ê¸°**: (1ë‹¨ê³„, 2ë‹¨ê³„... êµ¬ì²´ì  ì•ˆë‚´)
ğŸ’¡ **ì¶”ê°€ íŒ**: (ì‹¤ìˆ˜í•˜ê¸° ì‰¬ìš´ ë¶€ë¶„ì´ë‚˜ ìœ ìš©í•œ íŒ)
ğŸ¤” **ë” ê¶ê¸ˆí•˜ë‹¤ë©´**: (ê´€ë ¨í•´ì„œ ë” ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” ê²ƒë“¤)

ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ, ë§ˆì¹˜ ì˜†ì—ì„œ ì§ì ‘ ë„ì™€ì£¼ëŠ” ê²ƒì²˜ëŸ¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
            
            if answer_style == "concise":
                prompt += "\n\n[ìŠ¤íƒ€ì¼] í•µì‹¬ë§Œ ê°„ë‹¨íˆ 3ë‹¨ê³„ ì´ë‚´"
            
            # Use Gemini 2.0 Flash for beginner-friendly responses (cost-effective and gentle)
            if self.gemini_2_0_flash_model:
                return await self._call_gemini_2_0_flash(prompt, temperature=0.4)
            elif self.gemini_2_5_flash_model:
                return await self._call_gemini_2_5_flash(prompt, temperature=0.4)
            else:
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.4)
            
        except Exception as e:
            raise AIServiceException(f"ì´ˆë³´ì ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def handle_problem_report(self, previous_answer: str, user_feedback: str, context: str = "") -> str:
        """Handle 'problem report' button functionality for answer refinement"""
        try:
            prompt = f"""ì‚¬ìš©ìê°€ ì´ì „ ë‹µë³€ì— ë¬¸ì œê°€ ìˆë‹¤ê³  ë³´ê³ í–ˆìŠµë‹ˆë‹¤. ê°œì„ ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

**ì´ì „ ë‹µë³€**: {previous_answer}
**ì‚¬ìš©ì í”¼ë“œë°±**: {user_feedback}
**ëŒ€í™” ë§¥ë½**: {context}

**ê°œì„  ìš”ì²­ì‚¬í•­**:
1. ì´ì „ ë‹µë³€ì˜ ë¬¸ì œì  ë¶„ì„
2. ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜
3. ë” ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ í•´ê²°ì±… ì œì‹œ
4. ë‹¨ê³„ë³„ ê²€ì¦ ë°©ë²• í¬í•¨

**ì¶œë ¥ í˜•ì‹**:
ğŸ” **ë¬¸ì œì  ë¶„ì„** (ì´ì „ ë‹µë³€ì˜ ë¶€ì¡±í•œ ë¶€ë¶„)
âœ… **ê°œì„ ëœ í•´ê²°ì±…** (ìˆ˜ì •ëœ ë°©ë²•)
ğŸ“‹ **ë‹¨ê³„ë³„ ê°€ì´ë“œ** (êµ¬ì²´ì  ì‹¤í–‰ ë°©ë²•)
ğŸ§ª **ê²€ì¦ ë°©ë²•** (ê²°ê³¼ í™•ì¸ ë°©ë²•)
âš ï¸ **ì£¼ì˜ì‚¬í•­** (ì‹¤í–‰ ì‹œ ì£¼ì˜ì )

ë” ì‹ ì¤‘í•˜ê³  ê²€ì¦ëœ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""
            
            # Use hybrid approach for problem resolution (highest quality)
            if self.gemini_pro_model and self.openai_client:
                # Step 1: Gemini 2.5 Pro for analysis
                analysis_response = await self._call_gemini_pro(prompt, temperature=0.3)
                
                # Step 2: OpenAI for refinement
                refinement_prompt = f"""ë‹¤ìŒ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ê°œì„ ì•ˆì„ ì œì‹œí•˜ì„¸ìš”:

{analysis_response}

ì‚¬ìš©ìì˜ êµ¬ì²´ì  ìš”êµ¬ì‚¬í•­ì„ ì™„ì „íˆ ì¶©ì¡±í•˜ëŠ” ì‹¤ìš©ì ì¸ í•´ê²°ì±…ì„ ì œê³µí•˜ì„¸ìš”."""
                
                refined_response = await self._call_openai(refinement_prompt, model=settings.OPENAI_MODEL, temperature=0.2)
                
                return f"""ğŸš¨ **ë¬¸ì œ í•´ê²° - ê°œì„ ëœ ë‹µë³€**

{refined_response}

---
*ì´ ë‹µë³€ì€ ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ Gemini 2.5 Proì™€ OpenAIê°€ í˜‘ë ¥í•˜ì—¬ ê°œì„ í–ˆìŠµë‹ˆë‹¤.*"""
                
            elif self.gemini_pro_model:
                return await self._call_gemini_pro(prompt, temperature=0.3)
            else:
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.3)
            
        except Exception as e:
            raise AIServiceException(f"ë¬¸ì œ í•´ê²° ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    
    async def generate_debugging_response(self, context: str, feedback: str, image_data: Optional[bytes] = None) -> str:
        """Generate debugging response using OpenAI (optimized for problem-solving)"""
        try:
            # If image is provided, use Gemini 2.0 Flash for image analysis
            image_analysis = ""
            if image_data:
                if self.gemini_2_0_flash_model or self.gemini_2_5_flash_model:
                    try:
                        image_analysis = await self._analyze_image_with_gemini(image_data)
                    except Exception as e:
                        image_analysis = f"[ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}]"
                else:
                    image_analysis = "[ì´ë¯¸ì§€ê°€ ì²¨ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. VLOOKUP ì„œì‹ ë¶ˆì¼ì¹˜ ë¬¸ì œë¡œ ì¶”ì •ë©ë‹ˆë‹¤.]"
            
            # Add debugging persona prompt with feedback acknowledgment
            debugging_prompt = f"""ì•„ì´ê³ , ì œê°€ ë“œë¦° ë°©ë²•ì´ í†µí•˜ì§€ ì•Šì•˜êµ°ìš”. ì •ë§ ì£„ì†¡í•´ìš”! ğŸ˜­ ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ì˜ ë°›ì•˜ê³ , í•¨ê»˜ ë¬¸ì œë¥¼ í•´ê²°í•´ ë³´ë„ë¡ í• ê²Œìš”.

{DEBUGGING_PERSONA_PROMPT}

--- Previous Context ---
{context}

--- User Feedback ---
{feedback}

--- Image Analysis ---
{image_analysis}"""
            
            # Use OpenAI for debugging (best at problem-solving and code analysis)
            return await self._call_openai(debugging_prompt, model=settings.OPENAI_MODEL, temperature=0.5)
            
        except Exception as e:
            raise AIServiceException(f"ë””ë²„ê¹… ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def _call_openai(self, prompt: str, model: str, temperature: float = 0.7) -> str:
        """Make OpenAI API call with enhanced error handling"""
        if not self.openai_client:
            raise AIServiceException("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            response = await asyncio.wait_for(
                self.openai_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=temperature,
                    max_tokens=settings.MAX_TOKENS
                ),
                timeout=settings.AI_REQUEST_TIMEOUT
            )
            
            return response.choices[0].message.content.strip()
            
        except asyncio.TimeoutError:
            raise AIServiceException("OpenAI ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
        except Exception as e:
            raise AIServiceException(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def _call_gemini_pro(self, prompt: str, temperature: float = 0.7) -> str:
        """Make Gemini 2.5 Pro API call (highest difficulty)"""
        if not self.gemini_pro_model:
            print("âŒ Gemini 2.5 Pro ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            raise AIServiceException("Gemini 2.5 Pro ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            print(f"ğŸš€ Gemini 2.5 Pro API í˜¸ì¶œ ì‹œì‘ (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì)")
            response = await asyncio.wait_for(
                self.gemini_pro_model.generate_content_async(prompt),
                timeout=settings.AI_REQUEST_TIMEOUT
            )
            print(f"âœ… Gemini 2.5 Pro ì‘ë‹µ ì„±ê³µ (ì‘ë‹µ ê¸¸ì´: {len(response.text)} ë¬¸ì)")
            return response.text.strip()
        except asyncio.TimeoutError:
            print("â° Gemini 2.5 Pro ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
            raise AIServiceException("Gemini 2.5 Pro ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
        except Exception as e:
            print(f"âŒ Gemini 2.5 Pro API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise AIServiceException(f"Gemini 2.5 Pro API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def _call_gemini_2_5_flash(self, prompt: str, temperature: float = 0.7) -> str:
        """Make Gemini 2.5 Flash API call (medium difficulty)"""
        if not self.gemini_2_5_flash_model:
            print("âŒ Gemini 2.5 Flash ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            raise AIServiceException("Gemini 2.5 Flash ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            print(f"ğŸš€ Gemini 2.5 Flash API í˜¸ì¶œ ì‹œì‘ (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì)")
            response = await asyncio.wait_for(
                self.gemini_2_5_flash_model.generate_content_async(prompt),
                timeout=45  # Medium timeout
            )
            print(f"âœ… Gemini 2.5 Flash ì‘ë‹µ ì„±ê³µ (ì‘ë‹µ ê¸¸ì´: {len(response.text)} ë¬¸ì)")
            return response.text.strip()
        except asyncio.TimeoutError:
            print("â° Gemini 2.5 Flash ì‘ë‹µ ì‹œê°„ ì´ˆê³¼, 2.0 Flashë¡œ í´ë°±")
            # Fallback to 2.0 Flash
            try:
                return await self._call_gemini_2_0_flash(prompt, temperature)
            except Exception as e2:
                print(f"âŒ Gemini 2.5 Flash í´ë°± ì‹¤íŒ¨: {str(e2)}")
                raise AIServiceException(f"Gemini 2.5 Flash ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ ë° í´ë°± ì‹¤íŒ¨: {str(e2)}")
        except Exception as e:
            print(f"âŒ Gemini 2.5 Flash API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}, 2.0 Flashë¡œ í´ë°±")
            # Fallback to 2.0 Flash
            try:
                return await self._call_gemini_2_0_flash(prompt, temperature)
            except Exception as e2:
                print(f"âŒ Gemini 2.5 Flash í´ë°± ì‹¤íŒ¨: {str(e2)}")
                raise AIServiceException(f"Gemini 2.5 Flash API ì‹¤íŒ¨ ë° í´ë°± ì‹¤íŒ¨: {str(e2)}")
    
    async def _call_gemini_2_0_flash(self, prompt: str, temperature: float = 0.7) -> str:
        """Make Gemini 2.0 Flash API call (general difficulty)"""
        if not self.gemini_2_0_flash_model:
            print("âŒ Gemini 2.0 Flash ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            raise AIServiceException("Gemini 2.0 Flash ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            print(f"ğŸš€ Gemini 2.0 Flash API í˜¸ì¶œ ì‹œì‘ (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì)")
            response = await asyncio.wait_for(
                self.gemini_2_0_flash_model.generate_content_async(prompt),
                timeout=30  # Fast timeout
            )
            print(f"âœ… Gemini 2.0 Flash ì‘ë‹µ ì„±ê³µ (ì‘ë‹µ ê¸¸ì´: {len(response.text)} ë¬¸ì)")
            return response.text.strip()
        except asyncio.TimeoutError:
            print("â° Gemini 2.0 Flash ì‘ë‹µ ì‹œê°„ ì´ˆê³¼, OpenAIë¡œ í´ë°±")
            # Fallback to OpenAI
            try:
                return await self._call_openai(prompt, model="gpt-4o-mini", temperature=temperature)
            except Exception as e2:
                print(f"âŒ Gemini 2.0 Flash OpenAI í´ë°± ì‹¤íŒ¨: {str(e2)}")
                raise AIServiceException(f"Gemini 2.0 Flash ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ ë° OpenAI í´ë°± ì‹¤íŒ¨: {str(e2)}")
        except Exception as e:
            print(f"âŒ Gemini 2.0 Flash API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}, OpenAIë¡œ í´ë°±")
            # Fallback to OpenAI
            try:
                return await self._call_openai(prompt, model="gpt-4o-mini", temperature=temperature)
            except Exception as e2:
                print(f"âŒ Gemini 2.0 Flash OpenAI í´ë°± ì‹¤íŒ¨: {str(e2)}")
                raise AIServiceException(f"Gemini 2.0 Flash API ì‹¤íŒ¨ ë° OpenAI í´ë°± ì‹¤íŒ¨: {str(e2)}")
    
    async def _analyze_image_with_gemini(self, image_data: bytes) -> str:
        """Analyze Excel image using optimized Gemini Flash models with enhanced prompts"""
        # ìš°ì„ ìˆœìœ„: Gemini 2.0 Flash > Gemini 2.5 Flash > Gemini Pro
        # Flash ëª¨ë¸ì´ ì´ë¯¸ì§€ ë¶„ì„ì— ìµœì í™”ë˜ì–´ ìˆìŒ
        model_to_use = None
        
        # 1ìˆœìœ„: Gemini 2.0 Flash (ê°€ì¥ ë¹ ë¥´ê³  ë¹„ìš© íš¨ìœ¨ì  - ê¸°ì´ˆ ì´ë¯¸ì§€ ì²˜ë¦¬)
        if self.gemini_2_0_flash_model:
            model_to_use = self.gemini_2_0_flash_model
            print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„: Gemini 2.0 Flash ì‚¬ìš© (ê¸°ì´ˆ ì´ë¯¸ì§€ ì²˜ë¦¬)")
        # 2ìˆœìœ„: Gemini 2.5 Flash (ë” ì •í™•í•œ ë¶„ì„)
        elif self.gemini_2_5_flash_model:
            model_to_use = self.gemini_2_5_flash_model
            print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„: Gemini 2.5 Flash ì‚¬ìš©")
        # 3ìˆœìœ„: Gemini Pro (ìµœê³  í’ˆì§ˆ, í•˜ì§€ë§Œ ëŠë¦¼)
        elif self.gemini_pro_model:
            model_to_use = self.gemini_pro_model
            print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„: Gemini Pro ì‚¬ìš©")
        
        if not model_to_use:
            return "[ì´ë¯¸ì§€ ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.]"
        
        try:
            image = Image.open(io.BytesIO(image_data))
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ í¬ê¸°: {image.width}x{image.height}, í˜•ì‹: {image.format}")
            
            # ì´ë¯¸ì§€ ìµœì í™” (Flash ëª¨ë¸ì— ìµœì í™”)
            if image.width > 1920 or image.height > 1080:
                image.thumbnail((1920, 1080), Image.Resampling.LANCZOS)
                print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •: {image.width}x{image.height}")
            
            # Flash ëª¨ë¸ì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸
            prompt = """Excel í™”ë©´ ì´ë¯¸ì§€ë¥¼ ì •í™•í•˜ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”:

**ë¶„ì„ ìš”ì²­ì‚¬í•­**:
1. í™”ë©´ì— í‘œì‹œëœ Excel ê¸°ëŠ¥ì´ë‚˜ ì˜¤ë¥˜ ë©”ì‹œì§€ ì‹ë³„
2. ë°ì´í„° êµ¬ì¡°ì™€ ì…€ ë‚´ìš© íŒŒì•…
3. ë°œê²¬ëœ ë¬¸ì œì ê³¼ ì˜¤ë¥˜ ì›ì¸ ë¶„ì„
4. êµ¬ì²´ì ì¸ í•´ê²° ë°©ë²• ì œì‹œ

**ì¶œë ¥ í˜•ì‹**:
ğŸ–¼ï¸ **í™”ë©´ ë‚´ìš©**: (Excel ê¸°ëŠ¥, ì‹œíŠ¸ëª…, ì£¼ìš” ìš”ì†Œ)
ğŸ“‹ **ë°ì´í„° ë¶„ì„**: (ì…€ ë²”ìœ„, ë°ì´í„° íƒ€ì…, ìˆ˜ì‹ ë“±)
âš ï¸ **ë°œê²¬ëœ ë¬¸ì œ**: (ì˜¤ë¥˜ ë©”ì‹œì§€, ì„œì‹ ë¶ˆì¼ì¹˜, ê³µë°± ë“±)
ğŸ”§ **í•´ê²° ë°©ì•ˆ**: (ë‹¨ê³„ë³„ í•´ê²°ì±…, ìˆ˜ì‹ ìˆ˜ì •, ì„¤ì • ë³€ê²½ ë“±)
ğŸ’¡ **ì¶”ê°€ ì œì•ˆ**: (ìµœì í™” ë°©ë²•, ì£¼ì˜ì‚¬í•­)

íŠ¹íˆ VLOOKUP, INDEX/MATCH, ì¡°ê±´ë¶€ ì„œì‹, ë°ì´í„° ê²€ì¦ ê´€ë ¨ ë¬¸ì œë¥¼ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”."""
            
            # Flash ëª¨ë¸ì— ìµœì í™”ëœ íƒ€ì„ì•„ì›ƒ ì„¤ì •
            timeout = 30 if model_to_use == self.gemini_2_0_flash_model else 45
            
            response = await asyncio.wait_for(
                model_to_use.generate_content_async([prompt, image]),
                timeout=timeout
            )
            
            result = response.text.strip()
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {len(result)} ë¬¸ì")
            return result
            
        except asyncio.TimeoutError:
            print("â° ì´ë¯¸ì§€ ë¶„ì„ ì‹œê°„ ì´ˆê³¼")
            return "[ì´ë¯¸ì§€ ë¶„ì„ ì‹œê°„ ì´ˆê³¼: ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ë³µì¡í•˜ê±°ë‚˜ í¬ê¸°ê°€ í½ë‹ˆë‹¤.]"
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return f"[ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}. ì´ë¯¸ì§€ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.]"
    
    async def process_chat_request(
        self, 
        question: str, 
        context: str, 
        file_summary: str = "",
        is_feedback: bool = False,
        image_data: Optional[bytes] = None,
        conversation_context: Optional[Any] = None,
        answer_style: Optional[str] = None
    ) -> AIResponse:
        """Process complete chat request with intelligent model routing"""
        start_time = time.time()
        
        try:
            if is_feedback:
                # Handle feedback with debugging persona
                answer = await self.generate_debugging_response(context, question, image_data)
                model_used = "OpenAI GPT-4o"
                response_type = "normal"
                next_action = None
                conversation_state = None
            else:
                # Special flow: if a sheet was selected and we have file summary, provide detailed analysis
                if (file_summary and file_summary.strip()) and (
                    (not question) or 
                    question.strip().startswith("[ì‹œíŠ¸ì„ íƒ]") or
                    (len(question.strip()) <= 2 and question.strip().isdigit())
                ):
                    # Extract sheet name from file summary
                    sheet_name = "ì„ íƒëœ ì‹œíŠ¸"
                    if "ì„ íƒëœ ì‹œíŠ¸: '" in file_summary:
                        start_idx = file_summary.find("ì„ íƒëœ ì‹œíŠ¸: '") + len("ì„ íƒëœ ì‹œíŠ¸: '")
                        end_idx = file_summary.find("'", start_idx)
                        if end_idx > start_idx:
                            sheet_name = file_summary[start_idx:end_idx]
                    
                    answer = f"""ğŸ“Š **{sheet_name} ì‹œíŠ¸ ë¶„ì„ ì™„ë£Œ!**

{file_summary}

---

ğŸ¯ **ì´ì œ ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?**

1ï¸âƒ£ **ìˆ˜ì‹/í•¨ìˆ˜ ë§Œë“¤ê¸°** - VLOOKUP, SUMIF, INDEX/MATCH ë“±
2ï¸âƒ£ **ë°ì´í„° ì •ë¦¬** - ì¤‘ë³µ ì œê±°, ì •ë ¬, í•„í„°ë§
3ï¸âƒ£ **ìš”ì•½/ë¶„ì„** - í”¼ë²— í…Œì´ë¸”, í†µê³„ ë¶„ì„
4ï¸âƒ£ **ì‹œê°í™”** - ì°¨íŠ¸, ê·¸ë˜í”„ ë§Œë“¤ê¸°
5ï¸âƒ£ **ìë™í™”** - VBA ë§¤í¬ë¡œ, ë°˜ë³µ ì‘ì—… ìë™í™”

ë²ˆí˜¸ë¡œ ë‹µí•˜ì‹œê±°ë‚˜, êµ¬ì²´ì ìœ¼ë¡œ ì›í•˜ì‹œëŠ” ì‘ì—…ì„ ë§ì”€í•´ ì£¼ì„¸ìš”!
ì˜ˆ: "Aì—´ì˜ ì¤‘ë³µì„ ì œê±°í•˜ê³  Bì—´ë¡œ ì •ë ¬í•´ì¤˜" ë˜ëŠ” "ë§¤ì¶œ ë°ì´í„°ë¡œ í”¼ë²— í…Œì´ë¸” ë§Œë“¤ì–´ì¤˜"
"""
                    model_used = "conversation"
                    response_type = "clarification"
                    next_action = "wait_for_clarification"
                    conversation_state = ConversationState.CLARIFYING
                    
                    processing_time = time.time() - start_time
                    return AIResponse(
                        answer=answer,
                        session_id="",
                        model_used=model_used,
                        processing_time=processing_time,
                        response_type=response_type,
                        next_action=next_action,
                        conversation_state=conversation_state
                    )
                
                # Check if user selected a task option (1-5)
                elif file_summary and file_summary.strip() and question.strip() in ["1", "2", "3", "4", "5"]:
                    # User selected a task option
                    task_map = {
                        "1": "ìˆ˜ì‹/í•¨ìˆ˜ ë§Œë“¤ê¸°",
                        "2": "ë°ì´í„° ì •ë¦¬", 
                        "3": "ìš”ì•½/ë¶„ì„",
                        "4": "ì‹œê°í™”",
                        "5": "ìë™í™”"
                    }
                    selected_task = task_map[question.strip()]
                    
                    answer = f"""ğŸ¯ **{selected_task} ì‘ì—…ì„ ì„ íƒí•˜ì…¨ìŠµë‹ˆë‹¤!**

{self._get_task_examples(selected_task)}

**êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì‘ì—…ì„ ì›í•˜ì‹œë‚˜ìš”?**
ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ìì„¸íˆ ë§ì”€í•´ ì£¼ì„¸ìš”.

ì˜ˆ: "VLOOKUPìœ¼ë¡œ ë‹¤ë¥¸ ì‹œíŠ¸ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°" ë˜ëŠ” "ì¤‘ë³µ ë°ì´í„° ì œê±°í•˜ê³  ì •ë ¬í•˜ê¸°"
"""
                    model_used = "conversation"
                    response_type = "clarification"
                    next_action = "wait_for_task_details"
                    conversation_state = ConversationState.CLARIFYING
                    
                    processing_time = time.time() - start_time
                    return AIResponse(
                        answer=answer,
                        session_id="",
                        model_used=model_used,
                        processing_time=processing_time,
                        response_type=response_type,
                        next_action=next_action,
                        conversation_state=conversation_state
                    )
                
                # Process with AI based on question complexity
                else:
                    # Check for file generation request first
                    if self._is_file_generation_request(question):
                        print("ğŸ“ íŒŒì¼ ìƒì„± ìš”ì²­ ê°ì§€ë¨")
                        # Use Gemini 2.5 Pro for file generation (complex analysis)
                        ai_response = await self._generate_solution_with_context(
                            question, question, context, file_summary
                        )
                        answer = ai_response.answer
                        model_used = ai_response.model_used
                        response_type = "file_generation"
                        next_action = "generate_file"
                        conversation_state = ai_response.conversation_state
                        model_info = ai_response.model_info
                    elif image_data:
                        # ì´ë¯¸ì§€ê°€ ì²¨ë¶€ëœ Excel ì§ˆë¬¸ ì²˜ë¦¬
                        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ê°€ ì²¨ë¶€ëœ Excel ì§ˆë¬¸: ì´ë¯¸ì§€ ë¶„ì„ + Excel í•´ê²°ì±… ì œê³µ")
                        
                        # ì´ë¯¸ì§€ ë¶„ì„ ìˆ˜í–‰
                        image_analysis = await self._analyze_image_with_gemini(image_data)
                        
                        # Excel ì§ˆë¬¸ì— ëŒ€í•œ í•´ê²°ì±… ìƒì„±
                        enhanced_question = f"{question}\n\n[ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼]\n{image_analysis}"
                        
                        # Excel í•´ê²°ì±… ìƒì„± (ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ í¬í•¨)
                        if self._is_complex_task(question):
                            print("ğŸš€ ë³µì¡í•œ Excel ì‘ì—…: Gemini 2.5 Pro ì‚¬ìš©")
                            ai_response = await self._generate_solution_with_context(
                                enhanced_question, enhanced_question, context, file_summary
                            )
                            answer = ai_response.answer
                            model_used = ai_response.model_used
                            response_type = "solution_with_image"
                            next_action = ai_response.next_action
                            conversation_state = ai_response.conversation_state
                            model_info = ai_response.model_info
                        else:
                            print("âš¡ ê¸°ë³¸ Excel ì‘ì—…: Gemini 2.5 Flash ì‚¬ìš©")
                            answer = await self.generate_coding_response(enhanced_question, context, file_summary, answer_style)
                            model_used = "Gemini 2.5 Flash"
                            response_type = "solution_with_image"
                            next_action = "complete"
                            conversation_state = ConversationState.COMPLETED
                    elif self._is_complex_task(question):
                        # Complex task - use Gemini 2.5 Pro
                        print("ğŸš€ ë³µì¡í•œ ì‘ì—… ê°ì§€: Gemini 2.5 Pro ì‚¬ìš©")
                        print(f"ğŸ” ì§ˆë¬¸ ë‚´ìš©: {question}")
                        ai_response = await self._generate_solution_with_context(
                            question, question, context, file_summary
                        )
                        answer = ai_response.answer
                        model_used = ai_response.model_used
                        response_type = ai_response.response_type
                        next_action = ai_response.next_action
                        conversation_state = ai_response.conversation_state
                        model_info = ai_response.model_info
                    else:
                        # Basic task - use Gemini 2.5 Flash
                        print("âš¡ ê¸°ë³¸ ì‘ì—… ê°ì§€: Gemini 2.5 Flash ì‚¬ìš©")
                        print(f"ğŸ” ì§ˆë¬¸ ë‚´ìš©: {question}")
                        answer = await self.generate_coding_response(question, context, file_summary, answer_style)
                        model_used = "Gemini 2.5 Flash"
                        response_type = "solution"
                        next_action = "complete"
                        conversation_state = ConversationState.COMPLETED
            
            # Calculate processing time
            processing_time = time.time() - start_time
            
            # Create model info if not provided
            if 'model_info' not in locals():
                model_info = {
                    "model_name": model_used.lower().replace(" ", "_"),
                    "model_type": model_used,
                    "processing_time": processing_time,
                    "classification": "basic" if "Flash" in model_used else "complex"
                }
            
            return AIResponse(
                answer=answer,
                session_id="",
                model_used=model_used,
                processing_time=processing_time,
                response_type=response_type,
                next_action=next_action,
                conversation_state=conversation_state,
                model_info=model_info
            )
            
        except Exception as e:
            print(f"âŒ process_chat_request ì˜¤ë¥˜: {e}")
            raise AIServiceException(f"ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    async def _process_standard_question(
        self, 
        question: str, 
        context: str, 
        file_summary: str,
        classification: QuestionClassification,
        image_data: Optional[bytes] = None,
        answer_style: Optional[str] = None
    ) -> str:
        """Process standard question without clarification needs"""
        # ì´ë¯¸ì§€ê°€ ì²¨ë¶€ëœ ê²½ìš° Flash ëª¨ë¸ ìš°ì„  ì‚¬ìš©
        if image_data:
            print("ğŸ–¼ï¸ ì´ë¯¸ì§€ê°€ ì²¨ë¶€ëœ ì§ˆë¬¸: Flash ëª¨ë¸ ìš°ì„  ì²˜ë¦¬")
            try:
                # ì´ë¯¸ì§€ ë¶„ì„ ìˆ˜í–‰
                image_analysis = await self._analyze_image_with_gemini(image_data)
                
                # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•œ ì§ˆë¬¸ ì²˜ë¦¬
                enhanced_question = f"{question}\n\n[ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼]\n{image_analysis}"
                
                # Flash ëª¨ë¸ë¡œ ì²˜ë¦¬ (ì´ë¯¸ì§€ ë¶„ì„ì— ìµœì í™”)
                if self.gemini_2_0_flash_model:
                    return await self._call_gemini_2_0_flash(enhanced_question, temperature=0.7)
                elif self.gemini_2_5_flash_model:
                    return await self._call_gemini_2_5_flash(enhanced_question, temperature=0.7)
                else:
                    # Flash ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
                    print("âš ï¸ Flash ëª¨ë¸ ì—†ìŒ: ê¸°ì¡´ ë¡œì§ ì‚¬ìš©")
            except Exception as e:
                print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}, ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ í´ë°±")
        
        # If question is specific enough, generate solution directly
        if self._is_specific_enough(question):
            return await self._generate_solution_with_context(
                question,  # Use the question as the task
                question,  # Use the question as understanding
                context,
                file_summary
            )
        
        # Otherwise, use classification-based processing with user-level awareness
        if classification.classification == "beginner_help":
            print(f"ğŸ¤– ë¶„ë¥˜: ì´ˆë³´ì ë„ì›€ - {classification.recommended_model} ì‚¬ìš©")
            return await self.generate_beginner_response(question, context, file_summary, answer_style)
        elif classification.classification == "coding":
            print(f"ğŸ”§ ë¶„ë¥˜: ê¸°ë³¸ ìˆ˜ì‹/ì½”ë”© - {classification.recommended_model} ì‚¬ìš©")
            return await self.generate_coding_response(question, context, file_summary, answer_style)
        elif classification.classification == "advanced_coding":
            print(f"ğŸš€ ë¶„ë¥˜: ê³ ê¸‰/ë³µì¡í•œ ì‘ì—… - {classification.recommended_model} ì‚¬ìš©")
            return await self.generate_advanced_coding_response(question, context, file_summary, answer_style)
        elif classification.classification == "analysis":
            print(f"ğŸ“Š ë¶„ë¥˜: ë°ì´í„° ë¶„ì„ - {classification.recommended_model} ì‚¬ìš©")
            return await self.generate_analysis_response(question, context, file_summary, answer_style)
        elif classification.classification == "planning":
            print(f"ğŸ“‹ ë¶„ë¥˜: ê³„íš ìˆ˜ë¦½ - {classification.recommended_model} ì‚¬ìš©")
            return await self.generate_planning_response(context, file_summary, answer_style)
        elif classification.classification == "simple":
            print(f"ğŸ’¡ ë¶„ë¥˜: ê¸°ë³¸ ì§ˆì˜ - {classification.recommended_model} ì‚¬ìš©")
            return await self.generate_simple_response(question, answer_style)
        elif classification.classification == "hybrid":
            print(f"ğŸ”„ ë¶„ë¥˜: í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ - {classification.recommended_model} ì‚¬ìš©")
            return await self.generate_hybrid_response(question, context, file_summary, answer_style)
        elif classification.classification == "continuation":
            print(f"ğŸ”— ë¶„ë¥˜: ëŒ€í™” ì—°ê²° - {classification.recommended_model} ì‚¬ìš©")
            return await self.generate_continuation_response(question, context, file_summary, answer_style)
        elif classification.classification == "debugging":
            print(f"ï¿½ï¿½ ë¶„ë¥˜: ë””ë²„ê¹… - OpenAI ì‚¬ìš©")
            return await self.generate_debugging_response(context, question, image_data)
        else:
            # Default to beginner help for unknown classifications
            print(f"â“ ë¶„ë¥˜: ì•Œ ìˆ˜ ì—†ìŒ - ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
            return await self.generate_beginner_response(question, context, file_summary, answer_style)
    
    async def _generate_solution_with_context(
        self,
        original_question: str,
        understanding: str,
        context: str,
        file_summary: str
    ) -> AIResponse:
        """Generate solution based on gathered context and understanding"""
        start_time = time.time()
        
        try:
            # Use Gemini 2.5 Pro for complex solutions
            if self.gemini_pro_model:
                print("ğŸš€ ë³µì¡í•œ ì†”ë£¨ì…˜ ìƒì„±: Gemini 2.5 Pro ì‚¬ìš©")
                
                # íŒŒì¼ ìƒì„± ìš”ì²­ì¸ì§€ í™•ì¸
                is_file_generation = self._is_file_generation_request(original_question)
                
                if is_file_generation:
                    prompt = f"""ì œê³µëœ Excel ì‹œíŠ¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

**ì§ˆë¬¸**: {original_question}
**ì‹œíŠ¸ ë°ì´í„°**: {file_summary}

**âš ï¸ ë§¤ìš° ì¤‘ìš”í•œ ì§€ì¹¨**:
1. **ë°˜ë“œì‹œ ì œê³µëœ ì‹œíŠ¸ ë°ì´í„°ì˜ ì‹¤ì œ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”**
2. **ê°€ìƒì˜ ì´ë¦„ì´ë‚˜ ë°ì´í„°ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”**
3. **ì›ë³¸ ë°ì´í„°ì˜ ì •í™•í•œ ê°’(í•™ìƒ 1, í•™ìƒ 2 ë“±)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”**
4. **ì‹¤ì œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° "ë°ì´í„° ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”**

**ì¶œë ¥ í˜•ì‹**:
ğŸ¯ **ë°ì´í„° ë¶„ì„** (ì œê³µëœ ë°ì´í„° ê¸°ë°˜ ë¶„ì„)
ğŸ”§ **êµ¬ì²´ì  ê³„ì‚°** (í•©ê³„, í‰ê· , ìˆœìœ„ ë“± ì‹¤ì œ ê³„ì‚°)
ğŸ“‹ **ì‹¤í–‰ ë‹¨ê³„** (Excelì—ì„œ ì§ì ‘ ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„)
ğŸ’¡ **ê²°ê³¼ í•´ì„** (ë¶„ì„ ê²°ê³¼ì˜ ì˜ë¯¸)
ğŸ“Š **ì‹œê°í™” ì œì•ˆ** (ì°¨íŠ¸/í”¼ë²—í…Œì´ë¸” ìƒì„± ë°©ë²•)

**íŒŒì¼ ìƒì„±**:
- ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ ë‹¤ìŒ ë§í¬ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”:

[ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ] analysis_result.xlsx

**ì£¼ì˜**: ì¼ë°˜ì ì¸ ì„¤ëª…ì´ ì•„ë‹Œ, ì œê³µëœ ë°ì´í„°ì— ì§ì ‘ ì ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”.
"""
                else:
                    prompt = f"""ì œê³µëœ Excel ì‹œíŠ¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.

**ì§ˆë¬¸**: {original_question}
**ì‹œíŠ¸ ë°ì´í„°**: {file_summary}

**âš ï¸ ë§¤ìš° ì¤‘ìš”í•œ ì§€ì¹¨**:
1. **ë°˜ë“œì‹œ ì œê³µëœ ì‹œíŠ¸ ë°ì´í„°ì˜ ì‹¤ì œ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”**
2. **ê°€ìƒì˜ ì´ë¦„ì´ë‚˜ ë°ì´í„°ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”**
3. **ì›ë³¸ ë°ì´í„°ì˜ ì •í™•í•œ ê°’(í•™ìƒ 1, í•™ìƒ 2 ë“±)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”**
4. **ì‹¤ì œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° "ë°ì´í„° ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”**

**ì¶œë ¥ í˜•ì‹**:
ğŸ¯ **ë°ì´í„° ë¶„ì„** (ì œê³µëœ ë°ì´í„° ê¸°ë°˜ ë¶„ì„)
ğŸ”§ **êµ¬ì²´ì  ê³„ì‚°** (í•©ê³„, í‰ê· , ìˆœìœ„ ë“± ì‹¤ì œ ê³„ì‚°)
ğŸ“‹ **ì‹¤í–‰ ë‹¨ê³„** (Excelì—ì„œ ì§ì ‘ ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„)
ğŸ’¡ **ê²°ê³¼ í•´ì„** (ë¶„ì„ ê²°ê³¼ì˜ ì˜ë¯¸)
ğŸ“Š **ì‹œê°í™” ì œì•ˆ** (ì°¨íŠ¸/í”¼ë²—í…Œì´ë¸” ìƒì„± ë°©ë²•)

**ì£¼ì˜**: ì¼ë°˜ì ì¸ ì„¤ëª…ì´ ì•„ë‹Œ, ì œê³µëœ ë°ì´í„°ì— ì§ì ‘ ì ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”.
"""
                
                try:
                    response = await self._call_gemini_pro(prompt, temperature=0.3)
                    print("âœ… Gemini 2.5 Pro ì†”ë£¨ì…˜ ìƒì„± ì„±ê³µ")
                    processing_time = time.time() - start_time
                    return AIResponse(
                        answer=response,
                        session_id="",
                        model_used="Gemini 2.5 Pro",
                        processing_time=processing_time,
                        response_type="solution",
                        next_action="complete",
                        conversation_state=ConversationState.COMPLETED,
                        model_info={
                            "model_name": "gemini-2.5-pro",
                            "model_type": "Gemini 2.5 Pro (ê³ ê¸‰/ë³µì¡í•œ ì‘ì—…)",
                            "processing_time": processing_time,
                            "classification": "complex"
                        }
                    )
                except Exception as e:
                    print(f"âŒ Gemini 2.5 Pro ì†”ë£¨ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
                    
                    # 1ì°¨ í´ë°±: Gemini 2.5 Flash ì‹œë„
                    try:
                        print("ğŸ”„ Gemini 2.5 Flashë¡œ 1ì°¨ í´ë°±")
                        fallback_response = await self._call_gemini_2_5_flash(prompt, temperature=0.3)
                        processing_time = time.time() - start_time
                        return AIResponse(
                            answer=fallback_response,
                            session_id="",
                            model_used="Gemini 2.5 Flash",
                            processing_time=processing_time,
                            response_type="solution",
                            next_action="complete",
                            conversation_state=ConversationState.COMPLETED,
                            model_info={
                                "model_name": "gemini-2.5-flash",
                                "model_type": "Gemini 2.5 Flash (1ì°¨ í´ë°±)",
                                "processing_time": processing_time,
                                "classification": "fallback"
                            }
                        )
                    except Exception as flash_error:
                        print(f"âŒ Gemini 2.5 Flash í´ë°± ì‹¤íŒ¨: {flash_error}")
                        
                        # 2ì°¨ í´ë°±: OpenAI
                        try:
                            print("ğŸ”„ OpenAIë¡œ 2ì°¨ í´ë°±")
                            fallback_response = await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.3)
                            processing_time = time.time() - start_time
                            return AIResponse(
                                answer=fallback_response,
                                session_id="",
                                model_used="OpenAI GPT-4o",
                                processing_time=processing_time,
                                response_type="solution",
                                next_action="complete",
                                conversation_state=ConversationState.COMPLETED,
                                model_info={
                                    "model_name": "gpt-4o-mini",
                                    "model_type": "OpenAI GPT-4o (2ì°¨ í´ë°±)",
                                    "processing_time": processing_time,
                                    "classification": "fallback"
                                }
                            )
                        except Exception as openai_error:
                            print(f"âŒ OpenAI í´ë°±ë„ ì‹¤íŒ¨: {openai_error}")
                            raise AIServiceException(f"ëª¨ë“  AI ëª¨ë¸ í´ë°± ì‹¤íŒ¨: {str(e)}")
            else:
                print("ğŸš€ ë³µì¡í•œ ì†”ë£¨ì…˜ ìƒì„±: OpenAI ì‚¬ìš© (Gemini Pro ì—†ìŒ)")
                prompt = f"""Excel ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”.

**ì§ˆë¬¸**: {original_question}
**íŒŒì¼ ì •ë³´**: {file_summary}

**ì¶œë ¥ í˜•ì‹**:
ğŸ¯ **ë¬¸ì œ ë¶„ì„** (ë¬¸ì œì ê³¼ ì›ì¸)
ğŸ”§ **í•´ê²° ë°©ë²•** (ë‹¨ê³„ë³„ ì ‘ê·¼)
ğŸ“‹ **êµ¬ì²´ì  ë‹¨ê³„** (ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„)
ğŸ’¡ **ì¶”ê°€ íŒ** (ì£¼ì˜ì‚¬í•­ê³¼ ìµœì í™”)
ğŸ“Š **ê²°ê³¼ í™•ì¸** (ì˜ˆìƒ ê²°ê³¼)

[ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ] analysis_result.xlsx"""
                response = await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.3)
                processing_time = time.time() - start_time
                return AIResponse(
                    answer=response,
                    session_id="",
                    model_used="OpenAI GPT-4o",
                    processing_time=processing_time,
                    response_type="solution",
                    next_action="complete",
                    conversation_state=ConversationState.COMPLETED,
                    model_info={
                        "model_name": "gpt-4o-mini",
                        "model_type": "OpenAI GPT-4o (ë³´ì¡°/ë¡¤ë°±)",
                        "processing_time": processing_time,
                        "classification": "fallback"
                    }
                )
                
        except Exception as e:
            print(f"âŒ ì†”ë£¨ì…˜ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            raise AIServiceException(f"ì†”ë£¨ì…˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    def _is_new_question(self, question: str, context: str) -> bool:
        """Check if the question is new or a continuation"""
        question_lower = question.lower()
        
        # Keywords that indicate a new question
        new_question_indicators = [
            "ì¶”ê°€", "ë˜", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ë‹¤ìŒ", "ì´ë²ˆì—ëŠ”", "ì´ì œ", "ìƒˆë¡œ",
            "ë‹¤ë¥¸", "ë³„ë„", "ì¶”ê°€ë¡œ", "ë¶€ê°€ë¡œ", "ê·¸ ë‹¤ìŒ", "ê·¸ë¦¬ê³  ë‚˜ì„œ",
            "eì—´", "fì—´", "gì—´", "ìƒˆ ì—´", "ë‹¤ë¥¸ ì—´", "ë‹¤ë¥¸ ì‹œíŠ¸", "ìƒˆ ì‹œíŠ¸"
        ]
        
        # Check if question contains new question indicators
        has_new_indicators = any(indicator in question_lower for indicator in new_question_indicators)
        
        # Check if question mentions different columns/sheets than previous context
        context_lower = context.lower()
        has_different_targets = self._has_different_targets(question_lower, context_lower)
        
        # Check if question is about a different function/operation
        has_different_operation = self._has_different_operation(question_lower, context_lower)
        
        return has_new_indicators or has_different_targets or has_different_operation
    
    def _has_different_targets(self, question: str, context: str) -> bool:
        """Check if question targets different columns/sheets than context"""
        # Extract column/sheet references
        import re
        
        # Find column references (Aì—´, Bì—´, etc.)
        question_cols = re.findall(r'[a-z]ì—´', question)
        context_cols = re.findall(r'[a-z]ì—´', context)
        
        # If question mentions columns not in context, it's likely new
        if question_cols and context_cols:
            return not any(col in context_cols for col in question_cols)
        
        return False
    
    def _has_different_operation(self, question: str, context: str) -> bool:
        """Check if question is about a different operation than context"""
        # Different function keywords
        functions = {
            "vlookup": ["vlookup", "vlookup"],
            "xlookup": ["xlookup", "xlookup"],
            "if": ["if", "ì¡°ê±´", "í™•ì¸"],
            "isnumber": ["isnumber", "ìˆ«ìì¸ì§€", "ìˆ«ì í™•ì¸"],
            "istext": ["istext", "ë¬¸ìì¸ì§€", "ë¬¸ì í™•ì¸"],
            "sumif": ["sumif", "ì¡°ê±´í•©ê³„"],
            "countif": ["countif", "ì¡°ê±´ê°œìˆ˜"]
        }
        
        # Find functions mentioned in question and context
        question_funcs = []
        context_funcs = []
        
        for func_name, keywords in functions.items():
            if any(keyword in question for keyword in keywords):
                question_funcs.append(func_name)
            if any(keyword in context for keyword in keywords):
                context_funcs.append(func_name)
        
        # If question mentions different functions than context, it's likely new
        if question_funcs and context_funcs:
            return not any(func in context_funcs for func in question_funcs)
        
        return False
    
    def _is_specific_task_request(self, question: str) -> bool:
        """Check if the question contains a specific task request"""
        question_lower = question.lower()
        
        # Check for specific function names
        function_keywords = [
            "vlookup", "xlookup", "hlookup", "index", "match", "sumif", "countif", 
            "averageif", "if", "and", "or", "concatenate", "left", "right", "mid",
            "len", "trim", "substitute", "replace", "find", "search", "date", "today",
            "now", "year", "month", "day", "weekday", "eomonth", "datedif",
            "isnumber", "istext", "isna", "isblank", "iserror", "ìˆ«ìì¸ì§€", "ë¬¸ìì¸ì§€", "í™•ì¸"
        ]
        
        # Check for specific operations
        operation_keywords = [
            "ì°¾ì•„ì„œ", "ê°€ì ¸ì™€", "ì—°ê²°", "í•©ê³„", "í‰ê· ", "ê°œìˆ˜", "ì •ë ¬", "í•„í„°", 
            "ì¤‘ë³µ ì œê±°", "ì •ë¦¬", "ë¶„ì„", "ìš”ì•½", "ê·¸ë˜í”„", "ì°¨íŠ¸", "ë§¤í¬ë¡œ", "ìë™í™”",
            "ì¡°ê±´ë¶€", "ì„œì‹", "ìˆ˜ì‹", "í•¨ìˆ˜", "ì½”ë“œ", "ìŠ¤í¬ë¦½íŠ¸", "vba",
            "í™•ì¸í•˜ê³ ", "í™•ì¸í•˜ê³  ì‹¶ì–´", "ì•Œê³  ì‹¶ì–´", "ì›í•´", "í•„ìš”í•´"
        ]
        
        # Check for specific Excel terms
        excel_keywords = [
            "ì—´", "í–‰", "ì…€", "ì‹œíŠ¸", "ì›Œí¬ë¶", "ë²”ìœ„", "í”¼ë²—", "í…Œì´ë¸”", "ë°ì´í„°",
            "ê°’", "ì°¸ì¡°", "ë§í¬", "ë³µì‚¬", "ë¶™ì—¬ë„£ê¸°", "ì‚½ì…", "ì‚­ì œ", "ì´ë™"
        ]
        
        # Check for column references (Aì—´, Bì—´, etc.)
        import re
        column_pattern = r'[a-z]ì—´'
        has_column_ref = bool(re.search(column_pattern, question_lower))
        
        # Check if question contains specific function or operation
        has_function = any(func in question_lower for func in function_keywords)
        has_operation = any(op in question_lower for op in operation_keywords)
        has_excel_term = any(term in question_lower for term in excel_keywords)
        
        # Consider it specific if it has:
        # 1. Function names, OR
        # 2. Column references with operations, OR
        # 3. Detailed operations with Excel terms
        return has_function or (has_column_ref and has_operation) or (has_operation and has_excel_term)
    
    def _is_vba_or_complex_request(self, question: str) -> bool:
        """Check if the question is about VBA or complex operations"""
        question_lower = question.lower()
        
        # VBA related keywords (complex programming)
        vba_keywords = [
            "vba", "ë§¤í¬ë¡œ", "ì½”ë“œ", "ìŠ¤í¬ë¦½íŠ¸", "í”„ë¡œê·¸ë¨", "ì„œë¸Œë£¨í‹´", "í•¨ìˆ˜",
            "for", "while", "if", "then", "else", "end if", "loop", "next", "dim", "set",
            "sub", "function", "call", "exit", "goto", "on error", "resume"
        ]
        
        # Complex operation keywords (advanced features)
        complex_keywords = [
            "í†µí•©", "í•©ì¹˜ê¸°", "ë³‘í•©", "ì—°ê²°", "ëª¨ë“ ", "ì „ì²´", "ì‹œíŠ¸", "íŒŒì¼", "ë…„ë„", "ì›”ë³„",
            "ë§¤ì¶œ", "ìë£Œ", "ë°ì´í„°", "ê´€ë¦¬", "ì •ë¦¬", "ë¶„ì„", "ìš”ì•½", "ì§‘ê³„", "í†µê³„",
            "ë³µì¡í•œ", "ê³ ê¸‰", "ê³ ê¸‰ ê¸°ëŠ¥", "ì¡°í•©", "ì—¬ëŸ¬", "ë‹¤ì¤‘", "ì—°ì‡„", "ì—°ê²°ëœ",
            "ì¡°ê±´ë¶€ ì„œì‹", "ë°ì´í„° ìœ íš¨ì„±", "í”¼ë²— í…Œì´ë¸”", "ì°¨íŠ¸", "ê·¸ë˜í”„", "ì‹œê°í™”",
            "ë°ì´í„° ëª¨ë¸", "ê´€ê³„", "ì™¸ë¶€ ë°ì´í„°", "ì¿¼ë¦¬", "sql", "ë°ì´í„°ë² ì´ìŠ¤",
            "ì›Œí¬ì‹œíŠ¸ í•¨ìˆ˜", "ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜", "udf", "add-in", "í”ŒëŸ¬ê·¸ì¸"
        ]
        
        # Check for VBA keywords
        has_vba_keywords = any(keyword in question_lower for keyword in vba_keywords)
        
        # Check for complex operation keywords
        has_complex_keywords = any(keyword in question_lower for keyword in complex_keywords)
        
        # Check for multiple operations or complex combinations
        operation_count = 0
        if "ê·¸ë¦¬ê³ " in question_lower or "ë˜í•œ" in question_lower or "ì¶”ê°€ë¡œ" in question_lower:
            operation_count += 1
        if "ì—¬ëŸ¬" in question_lower or "ë‹¤ì¤‘" in question_lower or "ë³µí•©" in question_lower:
            operation_count += 1
        if "ì‹œíŠ¸" in question_lower and ("ì—¬ëŸ¬" in question_lower or "ëª¨ë“ " in question_lower):
            operation_count += 1
            
        # Consider it complex if it has VBA keywords, complex keywords, or multiple operations
        return has_vba_keywords or has_complex_keywords or operation_count >= 2
    
    def _is_specific_enough(self, question: str) -> bool:
        """Check if the question is specific enough to process directly"""
        question_lower = question.lower()
        
        # Check for file generation requests (should be processed directly)
        file_generation_keywords = [
            "íŒŒì¼ìƒì„±", "íŒŒì¼ ìƒì„±", "íŒŒì¼ë¡œ", "íŒŒì¼ ë§Œë“¤ì–´", "íŒŒì¼ ë§Œë“¤ê¸°",
            "ì—‘ì…€íŒŒì¼", "ì—‘ì…€ íŒŒì¼", "ê²°ê³¼íŒŒì¼", "ê²°ê³¼ íŒŒì¼", "ë‹¤ìš´ë¡œë“œ", "ì €ì¥",
            "ë³´ê³ ì„œ", "ë¶„ì„ê²°ê³¼", "ë¶„ì„ ê²°ê³¼", "í†µê³„", "ì°¨íŠ¸", "ê·¸ë˜í”„", "ì‹œê°í™”",
            "í”¼ë²—í…Œì´ë¸”", "í”¼ë²— í…Œì´ë¸”", "ì¡°ê±´ë¶€ì„œì‹", "ì¡°ê±´ë¶€ ì„œì‹", "ë°ì´í„°ë¶„ì„",
            "ë°ì´í„° ë¶„ì„", "ì¸ì‚¬ì´íŠ¸", "ìµœì í™”", "í’ˆì§ˆì§„ë‹¨", "í’ˆì§ˆ ì§„ë‹¨"
        ]
        
        # If it's a file generation request, consider it specific enough
        if any(keyword in question_lower for keyword in file_generation_keywords):
            return True
        
        # Check for specific functions, operations, or targets
        specific_indicators = [
            # Functions
            "vlookup", "xlookup", "hlookup", "index", "match", "sumif", "countif",
            "averageif", "if", "and", "or", "concatenate", "left", "right", "mid",
            "len", "trim", "substitute", "replace", "find", "search", "date", "today",
            "now", "year", "month", "day", "weekday", "eomonth", "datedif",
            "isnumber", "istext", "isna", "isblank", "iserror", "ìˆ«ìì¸ì§€", "ë¬¸ìì¸ì§€", "í™•ì¸",
            
            # Operations
            "ì°¾ì•„ì„œ", "ê°€ì ¸ì™€", "ì—°ê²°", "í•©ê³„", "í‰ê· ", "ê°œìˆ˜", "ì •ë ¬", "í•„í„°",
            "ì¤‘ë³µ ì œê±°", "ì •ë¦¬", "ë¶„ì„", "ìš”ì•½", "ê·¸ë˜í”„", "ì°¨íŠ¸", "ë§¤í¬ë¡œ", "ìë™í™”",
            "ì¡°ê±´ë¶€", "ì„œì‹", "ìˆ˜ì‹", "í•¨ìˆ˜", "ì½”ë“œ", "ìŠ¤í¬ë¦½íŠ¸", "vba",
            "í™•ì¸í•˜ê³ ", "í™•ì¸í•˜ê³  ì‹¶ì–´", "ì•Œê³  ì‹¶ì–´", "ì›í•´", "í•„ìš”í•´",
            
            # Targets
            "aì—´", "bì—´", "cì—´", "dì—´", "eì—´", "fì—´", "gì—´", "hì—´", "iì—´", "jì—´",
            "1í–‰", "2í–‰", "3í–‰", "4í–‰", "5í–‰", "ì‹œíŠ¸", "íŒŒì¼", "ë°ì´í„°",
            
            # VBA and complex operations
            "vba", "ë§¤í¬ë¡œ", "ì½”ë“œ", "ìŠ¤í¬ë¦½íŠ¸", "ìë™í™”", "í”„ë¡œê·¸ë¨", "í•¨ìˆ˜", "ì„œë¸Œë£¨í‹´",
            "í†µí•©", "í•©ì¹˜ê¸°", "ë³‘í•©", "ì—°ê²°", "ëª¨ë“ ", "ì „ì²´", "ë…„ë„", "ì›”ë³„",
            "ë§¤ì¶œ", "ìë£Œ", "ê´€ë¦¬", "ì •ë¦¬", "ë¶„ì„", "ìš”ì•½", "ì§‘ê³„", "í†µê³„",
            "1ì›”", "2ì›”", "3ì›”", "4ì›”", "5ì›”", "6ì›”", "7ì›”", "8ì›”", "9ì›”", "10ì›”", "11ì›”", "12ì›”",
            "ê° ì´ë¦„", "ê° ì‹œíŠ¸", "ì—¬ëŸ¬ ì‹œíŠ¸", "ì—¬ëŸ¬ íŒŒì¼", "ë…„ë„ë³„", "ì›”ë³„"
        ]
        
        # Check if question contains specific indicators
        has_specific_indicators = any(indicator in question_lower for indicator in specific_indicators)
        
        # Check for column references (Aì—´, Bì—´, etc.)
        import re
        column_pattern = r'[a-z]ì—´'
        has_column_ref = bool(re.search(column_pattern, question_lower))
        
        # Check for specific patterns
        has_specific_patterns = any([
            "ì‹œíŠ¸ì— ì €ì¥" in question_lower,
            "íŒŒì¼ë¡œ ê´€ë¦¬" in question_lower,
            "í•œ ì‹œíŠ¸ì— í†µí•©" in question_lower,
            "ëª¨ë“  ë§¤ì¶œìë£Œ" in question_lower,
            "ë…„ë„ë³„ë¡œ" in question_lower,
            "vbaë¡œ" in question_lower,
            "ë§¤í¬ë¡œë¡œ" in question_lower
        ])
        
        return has_specific_indicators or has_column_ref or has_specific_patterns
    
    def _has_multiple_interpretations(self, question: str) -> bool:
        """Check if the question could have multiple interpretations"""
        question_lower = question.lower()
        
        # Ambiguous phrases that could mean different things
        ambiguous_phrases = [
            "ì •ë¦¬í•´ì¤˜",  # ë°ì´í„° ì •ë¦¬? ì„œì‹ ì •ë¦¬? 
            "ë¶„ì„í•´ì¤˜",  # ì–´ë–¤ ë¶„ì„?
            "ìš”ì•½í•´ì¤˜",  # ì–´ë–¤ ìš”ì•½?
            "ë§Œë“¤ì–´ì¤˜",  # ë¬´ì—‡ì„ ë§Œë“¤ê¹Œ?
            "í•´ì¤˜",      # ë„ˆë¬´ ì¼ë°˜ì 
            "ë„ì™€ì¤˜"     # ë„ˆë¬´ ì¼ë°˜ì 
        ]
        
        # Check if question contains only ambiguous phrases without specific context
        has_ambiguous = any(phrase in question_lower for phrase in ambiguous_phrases)
        
        # If it has ambiguous phrases but no specific context, it's unclear
        if has_ambiguous and not self._is_specific_enough(question):
            return True
            
        return False
    
    def _get_task_examples(self, task_type: str) -> str:
        """Get examples for specific task type"""
        examples = {
            "ìˆ˜ì‹/í•¨ìˆ˜ ë§Œë“¤ê¸°": """â€¢ VLOOKUPìœ¼ë¡œ ë‹¤ë¥¸ ì‹œíŠ¸ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
â€¢ SUMIFë¡œ ì¡°ê±´ì— ë§ëŠ” ê°’ í•©ê³„
â€¢ INDEX/MATCHë¡œ ìœ ì—°í•œ ë°ì´í„° ê²€ìƒ‰
â€¢ IF í•¨ìˆ˜ë¡œ ì¡°ê±´ë¶€ ê³„ì‚°
â€¢ COUNTIFë¡œ ì¡°ê±´ì— ë§ëŠ” ê°œìˆ˜ ì„¸ê¸°""",
            
            "ë°ì´í„° ì •ë¦¬": """â€¢ ì¤‘ë³µ ë°ì´í„° ì œê±°
â€¢ íŠ¹ì • ì—´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
â€¢ ë¹ˆ ì…€ ì •ë¦¬ ë° ì±„ìš°ê¸°
â€¢ ë°ì´í„° í˜•ì‹ í†µì¼
â€¢ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°""",
            
            "ìš”ì•½/ë¶„ì„": """â€¢ í”¼ë²— í…Œì´ë¸”ë¡œ ë°ì´í„° ìš”ì•½
â€¢ ë§¤ì¶œ/ì§€ì¶œ í†µê³„ ë¶„ì„
â€¢ ì›”ë³„/ë¶„ê¸°ë³„ ì§‘ê³„
â€¢ í‰ê· , ìµœëŒ€, ìµœì†Œê°’ ê³„ì‚°
â€¢ ë°ì´í„° íŒ¨í„´ ë¶„ì„""",
            
            "ì‹œê°í™”": """â€¢ ë§¤ì¶œ ì¶”ì´ ì„ ê·¸ë˜í”„
â€¢ ì¹´í…Œê³ ë¦¬ë³„ ë§‰ëŒ€ê·¸ë˜í”„
â€¢ ë¹„ìœ¨ ë¶„ì„ ì›í˜•ì°¨íŠ¸
â€¢ ë°ì´í„° ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
â€¢ ìƒê´€ê´€ê³„ ì‚°ì ë„""",
            
            "ìë™í™”": """â€¢ ë°˜ë³µ ì‘ì—… VBA ë§¤í¬ë¡œ
â€¢ ìë™ ë°ì´í„° ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â€¢ ì¡°ê±´ë¶€ ì„œì‹ ìë™ ì ìš©
â€¢ ì´ë©”ì¼ ìë™ ë°œì†¡
â€¢ íŒŒì¼ ìë™ ì €ì¥ ë° ë°±ì—…"""
        }
        return examples.get(task_type, "â€¢ êµ¬ì²´ì ì¸ ì‘ì—…ì„ ë§ì”€í•´ ì£¼ì„¸ìš”")
    
    def get_service_status(self) -> Dict[str, Any]:
        """Get comprehensive AI service status"""
        return {
            "openai_available": self.openai_client is not None,
            "gemini_pro_available": self.gemini_pro_model is not None,
            "gemini_2_5_flash_available": self.gemini_2_5_flash_model is not None,
            "gemini_2_0_flash_available": self.gemini_2_0_flash_model is not None,
            "models": {
                "openai": settings.OPENAI_MODEL,
                "gemini_pro": settings.GEMINI_PRO_MODEL,
                "gemini_2_5_flash": settings.GEMINI_2_5_FLASH_MODEL,
                "gemini_2_0_flash": settings.GEMINI_2_0_FLASH_MODEL
            },
            "capabilities": {
                "text_generation": True,
                "image_analysis": self.gemini_2_0_flash_model is not None,
                "fast_processing": self.gemini_2_0_flash_model is not None,
                "coding_excellence": self.gemini_pro_model is not None,
                "data_analysis": self.gemini_2_5_flash_model is not None,
                "planning": self.gemini_2_5_flash_model is not None,
                "hybrid_processing": self.openai_client is not None and self.gemini_pro_model is not None
            },
            "specialized_roles": {
                "beginner_help": "Gemini 2.0 Flash (ì´ˆë³´ì ì¹œí™”ì  ì„¤ëª…)",
                "coding": "Gemini 2.5 Flash (ê¸°ë³¸ í•¨ìˆ˜/ìˆ˜ì‹)",
                "analysis": "Gemini 2.5 Flash (ë¹ ë¥¸ ë°ì´í„° ë¶„ì„)",
                "planning": "Gemini 2.5 Flash (êµ¬ì¡°í™”ëœ ê³„íš ìˆ˜ë¦½)",
                "simple": "Gemini 2.0 Flash (ë¹„ìš© íš¨ìœ¨ì )",
                "complex": "Gemini 2.5 Pro (ê³ ê¸‰/ë³µì¡í•œ ì‘ì—…)",
                "image_analysis": "Gemini 2.0 Flash (ë©€í‹°ëª¨ë‹¬)",
                "auxiliary_support": "OpenAI (ë³´ì¡°ì  ì—­í• )"
            }
        }
    
    def _get_model_type(self, model_name: str) -> str:
        """Get human-readable model type for display"""
        if "gemini-2.5-pro" in model_name or "gemini_pro" in model_name:
            return "Gemini 2.5 Pro (ê³ ê¸‰/ë³µì¡í•œ ì‘ì—…)"
        elif "gemini-2.5-flash" in model_name or "gemini_2_5_flash" in model_name:
            return "Gemini 2.5 Flash (ê¸°ë³¸ ìˆ˜ì‹/ì½”ë”©)"
        elif "gemini-2.0-flash" in model_name or "gemini_2_0_flash" in model_name:
            return "Gemini 2.0 Flash (ê¸°ì´ˆ ì§ˆì˜/ì´ë¯¸ì§€)"
        elif "gpt-4o" in model_name or "openai" in model_name:
            return "OpenAI GPT-4o (ë³´ì¡°/ë¡¤ë°±)"
        elif "conversation" in model_name:
            return "ëŒ€í™” ê´€ë¦¬"
        else:
            return "ê¸°íƒ€ ëª¨ë¸"
    
    def _is_basic_task(self, question: str) -> bool:
        """Check if the question is a basic task that can be handled by Flash models"""
        question_lower = question.lower()
        
        # Basic function keywords (simple, single functions)
        basic_function_keywords = [
            "vlookup", "hlookup", "sumif", "countif", "averageif", "if", "and", "or",
            "left", "right", "mid", "len", "trim", "concatenate", "substitute", "replace",
            "isnumber", "istext", "isna", "isblank", "iserror", "ìˆ«ìì¸ì§€", "ë¬¸ìì¸ì§€",
            "sum", "average", "count", "max", "min", "round", "date", "today", "now"
        ]
        
        # Basic operation keywords (simple operations)
        basic_operation_keywords = [
            "ì°¾ì•„ì„œ", "ê°€ì ¸ì™€", "í•©ê³„", "í‰ê· ", "ê°œìˆ˜", "ì •ë ¬", "í•„í„°", "ì¤‘ë³µ ì œê±°",
            "í™•ì¸í•˜ê³ ", "ì•Œê³  ì‹¶ì–´", "ì›í•´", "í•„ìš”í•´", "ë§Œë“¤ì–´ì¤˜", "ì•Œë ¤ì¤˜", "ê³„ì‚°",
            "ë”í•˜ê¸°", "ë¹¼ê¸°", "ê³±í•˜ê¸°", "ë‚˜ëˆ„ê¸°", "ë°˜ì˜¬ë¦¼", "ì˜¬ë¦¼", "ë‚´ë¦¼"
        ]
        
        # Simple macro keywords (basic automation)
        simple_macro_keywords = [
            "ìë™", "ë°˜ë³µ", "ì¼ê´„", "ë³µì‚¬", "ë¶™ì—¬ë„£ê¸°", "ì„œì‹ ë³µì‚¬", "ìë™ ì±„ìš°ê¸°",
            "ê°„ë‹¨í•œ", "ê¸°ë³¸", "ë‹¨ìˆœ", "ìë™í™”"
        ]
        
        # Complex keywords that require Pro model
        complex_keywords = [
            "vba", "ë§¤í¬ë¡œ", "ì½”ë“œ", "ìŠ¤í¬ë¦½íŠ¸", "í”„ë¡œê·¸ë¨", "ì„œë¸Œë£¨í‹´", "í•¨ìˆ˜",
            "í†µí•©", "í•©ì¹˜ê¸°", "ë³‘í•©", "ì—°ê²°", "ëª¨ë“ ", "ì „ì²´", "ì‹œíŠ¸", "íŒŒì¼", "ë…„ë„", "ì›”ë³„",
            "ë§¤ì¶œ", "ìë£Œ", "ë°ì´í„°", "ê´€ë¦¬", "ì •ë¦¬", "ë¶„ì„", "ìš”ì•½", "ì§‘ê³„", "í†µê³„",
            "ë³µì¡í•œ", "ê³ ê¸‰", "ê³ ê¸‰ ê¸°ëŠ¥", "ì¡°í•©", "ì—¬ëŸ¬", "ë‹¤ì¤‘", "ì—°ì‡„", "ì—°ê²°ëœ",
            "ì¡°ê±´ë¶€ ì„œì‹", "ë°ì´í„° ìœ íš¨ì„±", "í”¼ë²— í…Œì´ë¸”", "ì°¨íŠ¸", "ê·¸ë˜í”„", "ì‹œê°í™”",
            "for", "while", "loop", "next", "dim", "set", "then", "else", "end if"
        ]
        
        # Check for complex keywords first
        has_complex_keywords = any(keyword in question_lower for keyword in complex_keywords)
        if has_complex_keywords:
            return False
        
        # Check for basic functions or operations
        has_basic_function = any(func in question_lower for func in basic_function_keywords)
        has_basic_operation = any(op in question_lower for op in basic_operation_keywords)
        has_simple_macro = any(macro in question_lower for macro in simple_macro_keywords)
        
        # Check for column references (Aì—´, Bì—´, etc.)
        import re
        column_pattern = r'[a-z]ì—´'
        has_column_ref = bool(re.search(column_pattern, question_lower))
        
        # Consider it basic if it has:
        # 1. Basic function names, OR
        # 2. Basic operations with column references, OR
        # 3. Simple operations without complex keywords, OR
        # 4. Simple macro requests
        return (has_basic_function or 
                (has_column_ref and has_basic_operation) or 
                has_basic_operation or 
                has_simple_macro)
    
    def _is_specific_task_request(self, question: str) -> bool:
        """Check if the question contains a specific task request"""
        question_lower = question.lower()
        
        # Check for specific function names
        function_keywords = [
            "vlookup", "xlookup", "hlookup", "index", "match", "sumif", "countif", 
            "averageif", "if", "and", "or", "concatenate", "left", "right", "mid",
            "len", "trim", "substitute", "replace", "find", "search", "date", "today",
            "now", "year", "month", "day", "weekday", "eomonth", "datedif",
            "isnumber", "istext", "isna", "isblank", "iserror", "ìˆ«ìì¸ì§€", "ë¬¸ìì¸ì§€", "í™•ì¸"
        ]
        
        # Check for specific operations
        operation_keywords = [
            "ì°¾ì•„ì„œ", "ê°€ì ¸ì™€", "ì—°ê²°", "í•©ê³„", "í‰ê· ", "ê°œìˆ˜", "ì •ë ¬", "í•„í„°", 
            "ì¤‘ë³µ ì œê±°", "ì •ë¦¬", "ë¶„ì„", "ìš”ì•½", "ê·¸ë˜í”„", "ì°¨íŠ¸", "ë§¤í¬ë¡œ", "ìë™í™”",
            "ì¡°ê±´ë¶€", "ì„œì‹", "ìˆ˜ì‹", "í•¨ìˆ˜", "ì½”ë“œ", "ìŠ¤í¬ë¦½íŠ¸", "vba",
            "í™•ì¸í•˜ê³ ", "í™•ì¸í•˜ê³  ì‹¶ì–´", "ì•Œê³  ì‹¶ì–´", "ì›í•´", "í•„ìš”í•´"
        ]
        
        # Check for specific Excel terms
        excel_keywords = [
            "ì—´", "í–‰", "ì…€", "ì‹œíŠ¸", "ì›Œí¬ë¶", "ë²”ìœ„", "í”¼ë²—", "í…Œì´ë¸”", "ë°ì´í„°",
            "ê°’", "ì°¸ì¡°", "ë§í¬", "ë³µì‚¬", "ë¶™ì—¬ë„£ê¸°", "ì‚½ì…", "ì‚­ì œ", "ì´ë™"
        ]
        
        # Check for column references (Aì—´, Bì—´, etc.)
        import re
        column_pattern = r'[a-z]ì—´'
        has_column_ref = bool(re.search(column_pattern, question_lower))
        
        # Check if question contains specific function or operation
        has_function = any(func in question_lower for func in function_keywords)
        has_operation = any(op in question_lower for op in operation_keywords)
        has_excel_term = any(term in question_lower for term in excel_keywords)
        
        # Consider it specific if it has:
        # 1. Function names, OR
        # 2. Column references with operations, OR
        # 3. Detailed operations with Excel terms
        return has_function or (has_column_ref and has_operation) or (has_operation and has_excel_term)
    
    def _is_file_generation_request(self, question: str) -> bool:
        """Check if the question is a file generation request - ë§¤ìš° ì—„ê²©í•œ ê¸°ì¤€"""
        question_lower = question.lower()
        
        # ë§¤ìš° ëª…ì‹œì ì¸ íŒŒì¼ ìƒì„± ìš”ì²­ í‚¤ì›Œë“œë§Œ ê°ì§€
        explicit_file_generation_keywords = [
            "íŒŒì¼ë¡œ ë§Œë“¤ì–´ì¤˜", "íŒŒì¼ ë§Œë“¤ì–´ì¤˜", "íŒŒì¼ë¡œ ë§Œë“¤ì–´ë‹¬ë¼", "íŒŒì¼ë¡œ ë§Œë“¤ì–´ë‹¬ë¼ê³ ",
            "íŒŒì¼ë¡œ ë§Œë“¤ì–´ë‹¬ë¼ê³ ìš”", "íŒŒì¼ë¡œ ë§Œë“¤ì–´ë‹¬ë¼êµ¬ìš”", "íŒŒì¼ë¡œ ë§Œë“¤ì–´ë‹¬ë¼êµ¬",
            "íŒŒì¼ìƒì„±í•´ì¤˜", "íŒŒì¼ ìƒì„±í•´ì¤˜", "íŒŒì¼ìƒì„±í•´ë‹¬ë¼", "íŒŒì¼ ìƒì„±í•´ë‹¬ë¼",
            "ë‹¤ìš´ë¡œë“œí•´ì¤˜", "ì €ì¥í•´ì¤˜", "íŒŒì¼ë¡œ ì €ì¥í•´ì¤˜", "íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì¤˜",
            "ì—‘ì…€íŒŒì¼ë¡œ ë§Œë“¤ì–´ì¤˜", "ì—‘ì…€ íŒŒì¼ë¡œ ë§Œë“¤ì–´ì¤˜", "ê²°ê³¼íŒŒì¼ë¡œ ë§Œë“¤ì–´ì¤˜", "ê²°ê³¼ íŒŒì¼ë¡œ ë§Œë“¤ì–´ì¤˜"
        ]
        
        # ë§¤ìš° ëª…ì‹œì ì¸ íŒŒì¼ ìƒì„± ìš”ì²­ë§Œ ê°ì§€ (ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ)
        for keyword in explicit_file_generation_keywords:
            if keyword in question_lower:
                print(f"âœ… íŒŒì¼ ìƒì„± ìš”ì²­ ê°ì§€: '{keyword}' í‚¤ì›Œë“œ ë°œê²¬")
                return True
        
        # ì¸ì½”ë”© ë¬¸ì œë¡œ ê¹¨ì§„ "íŒŒì¼ìƒì„±" ê°ì§€
        if "Ã­Å’Å’Ã¬" in question or "Ã¬Æ’" in question or "Ã¬â€Â±" in question:
            print("âœ… íŒŒì¼ ìƒì„± ìš”ì²­ ê°ì§€: ì¸ì½”ë”© ë¬¸ì œë¡œ ê¹¨ì§„ í‚¤ì›Œë“œ ë°œê²¬")
            return True
            
        print(f"âŒ íŒŒì¼ ìƒì„± ìš”ì²­ ì•„ë‹˜: '{question}'")
        return False
    
    def _is_complex_task(self, question: str) -> bool:
        """Check if the question is a complex task requiring Gemini 2.5 Pro"""
        question_lower = question.lower()
        
        # Complex keywords that require Pro model
        complex_keywords = [
            "vba", "ë§¤í¬ë¡œ", "ì½”ë“œ", "ìŠ¤í¬ë¦½íŠ¸", "í”„ë¡œê·¸ë¨", "ì„œë¸Œë£¨í‹´", "í•¨ìˆ˜",
            "í†µí•©", "í•©ì¹˜ê¸°", "ë³‘í•©", "ì—°ê²°", "ëª¨ë“ ", "ì „ì²´", "ì‹œíŠ¸", "ë…„ë„", "ì›”ë³„",
            "ë³µì¡í•œ", "ê³ ê¸‰", "ê³ ê¸‰ ê¸°ëŠ¥", "ì¡°í•©", "ì—¬ëŸ¬", "ë‹¤ì¤‘", "ì—°ì‡„", "ì—°ê²°ëœ",
            "ì¡°ê±´ë¶€ ì„œì‹", "ë°ì´í„° ìœ íš¨ì„±", "í”¼ë²— í…Œì´ë¸”", "ì°¨íŠ¸", "ê·¸ë˜í”„", "ì‹œê°í™”",
            "ë°ì´í„° ëª¨ë¸", "ê´€ê³„", "ì™¸ë¶€ ë°ì´í„°", "ì¿¼ë¦¬", "sql", "ë°ì´í„°ë² ì´ìŠ¤",
            "for", "while", "loop", "next", "dim", "set", "then", "else", "end if",
            # ê³ ê¸‰ Excel ê¸°ëŠ¥ í‚¤ì›Œë“œ ì¶”ê°€
            "í†µê³„ë¶„ì„", "í†µê³„ ë¶„ì„", "ë°ì´í„°ë¶„ì„", "ë°ì´í„° ë¶„ì„", "ì¸ì‚¬ì´íŠ¸", "ìµœì í™”",
            "í’ˆì§ˆì§„ë‹¨", "í’ˆì§ˆ ì§„ë‹¨", "ì´ìƒì¹˜", "ì´ìƒ ì¹˜", "ê²°ì¸¡ì¹˜", "ê²°ì¸¡ ì¹˜",
            "ìƒê´€ê´€ê³„", "ìƒê´€ ê´€ê³„", "ë¶„í¬", "ì™œë„", "ì²¨ë„", "ê¸°ìˆ í†µê³„", "ê¸°ìˆ  í†µê³„",
            "ë°ì´í„°ì‹œê°í™”", "ë°ì´í„° ì‹œê°í™”", "ëŒ€ì‹œë³´ë“œ", "ë¦¬í¬íŠ¸", "ë³´ê³ ì„œ"
        ]
        
        # Check for complex keywords
        has_complex_keywords = any(keyword in question_lower for keyword in complex_keywords)
        
        # Check for multiple operations
        operation_count = 0
        if "ê·¸ë¦¬ê³ " in question_lower or "ë˜í•œ" in question_lower or "ì¶”ê°€ë¡œ" in question_lower:
            operation_count += 1
        if "ì—¬ëŸ¬" in question_lower or "ë‹¤ì¤‘" in question_lower or "ë³µí•©" in question_lower:
            operation_count += 1
        if "ì‹œíŠ¸" in question_lower and ("ì—¬ëŸ¬" in question_lower or "ëª¨ë“ " in question_lower):
            operation_count += 1
            
        return has_complex_keywords or operation_count >= 2

# Global AI service instance
ai_service = AIService()
