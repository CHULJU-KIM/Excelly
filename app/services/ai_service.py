# app/services/ai_service.py
# AI model management service

import time
import asyncio
import json
from typing import Optional, Dict, Any, List, Tuple
from openai import AsyncOpenAI
import google.generativeai as genai
from PIL import Image
import io

from app.core.config import settings
from app.core.exceptions import AIServiceException
from app.models.chat import QuestionClassification, UserIntent, AIResponse, ConversationState
from prompts import PLANNING_PERSONA_PROMPT, CODING_PERSONA_PROMPT, DEBUGGING_PERSONA_PROMPT

class AIService:
    """Service for managing AI model interactions with intelligent routing"""
    
    def __init__(self):
        self.openai_client = None
        self.gemini_model = None
        self.gemini_flash_model = None
        self._initialize_clients()
    
    def _initialize_clients(self):
        """Initialize AI clients with optimized model selection"""
        try:
            # Initialize OpenAI client
            if settings.OPENAI_API_KEY:
                self.openai_client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
                print(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ: {settings.OPENAI_MODEL}")
            
            # Initialize Gemini client
            if settings.GEMINI_API_KEY:
                genai.configure(api_key=settings.GEMINI_API_KEY)
                
                # Initialize Gemini Pro model (for creative and analytical tasks)
                try:
                    self.gemini_model = genai.GenerativeModel(settings.GEMINI_PRO_MODEL)
                    print(f"âœ… Gemini 2.0 Pro ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ: {settings.GEMINI_PRO_MODEL}")
                except Exception as e:
                    print(f"âš ï¸ Gemini 2.0 Pro ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    # Fallback to Gemini 1.5 Pro
                    try:
                        self.gemini_model = genai.GenerativeModel(settings.GEMINI_1_5_PRO_FALLBACK)
                        print(f"âœ… Gemini 1.5 Pro ëª¨ë¸ë¡œ ëŒ€ì²´: {settings.GEMINI_1_5_PRO_FALLBACK}")
                    except Exception as e2:
                        print(f"âŒ Gemini Pro ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e2}")
                        self.gemini_model = None
                
                # Initialize Gemini Flash model (for fast processing and classification)
                try:
                    self.gemini_flash_model = genai.GenerativeModel(settings.GEMINI_FLASH_MODEL)
                    print(f"âœ… Gemini 2.0 Flash ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ: {settings.GEMINI_FLASH_MODEL}")
                except Exception as e:
                    print(f"âš ï¸ Gemini 2.0 Flash ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    # Fallback to Gemini 1.5 Flash
                    try:
                        self.gemini_flash_model = genai.GenerativeModel(settings.GEMINI_1_5_FLASH_FALLBACK)
                        print(f"âœ… Gemini 1.5 Flash ëª¨ë¸ë¡œ ëŒ€ì²´: {settings.GEMINI_1_5_FLASH_FALLBACK}")
                    except Exception as e2:
                        print(f"âŒ Gemini Flash ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e2}")
                        self.gemini_flash_model = None
                
        except Exception as e:
            print(f"âš ï¸ AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # Don't raise exception, continue with available models
    
    async def classify_question(self, question: str) -> QuestionClassification:
        """Enhanced question classification with optimized model selection"""
        try:
            prompt = f"""ë‹¤ìŒ ì‚¬ìš©ìž ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ AI ëª¨ë¸ê³¼ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •í•˜ì„¸ìš”.

ì§ˆë¬¸: "{question}"

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
1. **simple**: ê°„ë‹¨í•œ ì‚¬ì‹¤ ì§ˆë¬¸, í•¨ìˆ˜ ì‚¬ìš©ë²•, ê¸°ë³¸ ë¬¸ë²•
2. **complex**: ë³µìž¡í•œ ë¡œì§, ì—¬ëŸ¬ ë‹¨ê³„ ìž‘ì—…, íŒŒì¼ ë¶„ì„
3. **creative**: ì°½ì˜ì  ì•„ì´ë””ì–´, ìƒˆë¡œìš´ ì ‘ê·¼ë²•, ìµœì í™” ì œì•ˆ
4. **analytical**: ë°ì´í„° ë¶„ì„, íŒ¨í„´ ì°¾ê¸°, í†µê³„ì  ë¶„ì„
5. **debugging**: ì˜¤ë¥˜ í•´ê²°, ë¬¸ì œ ì§„ë‹¨, ì½”ë“œ ìˆ˜ì •

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”:
{{
    "classification": "simple|complex|creative|analytical|debugging",
    "confidence": 0.0-1.0,
    "reasoning": "ë¶„ë¥˜ ì´ìœ ",
    "recommended_model": "openai|gemini_pro|gemini_flash",
    "estimated_tokens": 100-4000
}}"""
            
            # Use Gemini 2.0 Flash for classification (fastest and most cost-effective)
            if self.gemini_flash_model:
                response = await self._call_gemini_flash(prompt, temperature=0.0)
            else:
                response = await self._call_openai(prompt, model="gpt-4o-mini", temperature=0.0)
            
            try:
                result = json.loads(response)
                return QuestionClassification(
                    classification=result.get("classification", "complex"),
                    confidence=result.get("confidence", 0.5),
                    reasoning=result.get("reasoning", ""),
                    recommended_model=result.get("recommended_model", "openai"),
                    estimated_tokens=result.get("estimated_tokens", 1000)
                )
            except json.JSONDecodeError:
                # Fallback parsing
                classification = self._fallback_classification(response)
                return QuestionClassification(
                    classification=classification,
                    confidence=0.7,
                    reasoning="JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ë¶„ë¥˜ ì‚¬ìš©",
                    recommended_model="openai",
                    estimated_tokens=1000
                )
                
        except Exception as e:
            return QuestionClassification(
                classification="complex",
                confidence=0.5,
                reasoning=f"ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}",
                recommended_model="openai",
                estimated_tokens=1000
            )
    
    def _fallback_classification(self, response: str) -> str:
        """Fallback classification when JSON parsing fails"""
        response_lower = response.lower()
        if any(word in response_lower for word in ["simple", "ê°„ë‹¨", "ê¸°ë³¸"]):
            return "simple"
        elif any(word in response_lower for word in ["creative", "ì°½ì˜", "ìƒˆë¡œìš´"]):
            return "creative"
        elif any(word in response_lower for word in ["analytical", "ë¶„ì„", "í†µê³„"]):
            return "analytical"
        elif any(word in response_lower for word in ["debugging", "ì˜¤ë¥˜", "ë¬¸ì œ"]):
            return "debugging"
        else:
            return "complex"
    
    async def analyze_user_intent(self, plan: str, user_reply: str) -> UserIntent:
        """Analyze user's intent using Gemini 2.0 Flash (optimized for fast analysis)"""
        try:
            prompt = f"""AIì˜ ê³„íš: "{plan[:200]}..."
ì‚¬ìš©ìž ë‹µë³€: "{user_reply}"

ì‚¬ìš©ìž ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”:
- "agreement": ê³„íšì— ë™ì˜í•˜ê³  ì§„í–‰
- "modification": ê³„íš ìˆ˜ì • ìš”ì²­
- "clarification": ì¶”ê°€ ì„¤ëª… ìš”ì²­
- "rejection": ê³„íš ê±°ë¶€

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”:
{{
    "intent": "agreement|modification|clarification|rejection",
    "confidence": 0.0-1.0,
    "reasoning": "ì˜ë„ ë¶„ì„ ì´ìœ "
}}"""
            
            # Use Gemini 2.0 Flash for intent analysis (fastest and most cost-effective)
            if self.gemini_flash_model:
                response = await self._call_gemini_flash(prompt, temperature=0.0)
            else:
                response = await self._call_openai(prompt, model="gpt-4o-mini", temperature=0.0)
            
            try:
                result = json.loads(response)
                return UserIntent(
                    intent=result.get("intent", "other"),
                    confidence=result.get("confidence", 0.5),
                    reasoning=result.get("reasoning", "")
                )
            except json.JSONDecodeError:
                intent = self._fallback_intent_analysis(response)
                return UserIntent(
                    intent=intent,
                    confidence=0.7,
                    reasoning="JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ë¶„ì„ ì‚¬ìš©"
                )
                
        except Exception as e:
            return UserIntent(
                intent="other",
                confidence=0.5,
                reasoning=f"ì˜ë„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            )
    
    def _fallback_intent_analysis(self, response: str) -> str:
        """Fallback intent analysis when JSON parsing fails"""
        response_lower = response.lower()
        if any(word in response_lower for word in ["agreement", "ë™ì˜", "ì¢‹ë‹¤", "ì§„í–‰"]):
            return "agreement"
        elif any(word in response_lower for word in ["modification", "ìˆ˜ì •", "ë³€ê²½"]):
            return "modification"
        elif any(word in response_lower for word in ["clarification", "ì„¤ëª…", "ì´í•´"]):
            return "clarification"
        elif any(word in response_lower for word in ["rejection", "ê±°ë¶€", "ì•„ë‹ˆ"]):
            return "rejection"
        else:
            return "other"
    
    async def generate_planning_response(self, context: str, file_summary: str = "") -> str:
        """Generate planning response using Gemini 2.0 Pro (optimized for structured thinking)"""
        try:
            prompt = f"{PLANNING_PERSONA_PROMPT}\n\n--- Conversation History ---\n{context}\n\n{file_summary}"
            
            # Use Gemini 2.0 Pro for planning (best at structured thinking and planning)
            if self.gemini_model:
                return await self._call_gemini(prompt, temperature=0.7)
            else:
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.7)
            
        except Exception as e:
            raise AIServiceException(f"ê³„íš ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_coding_response(self, context: str, task: str) -> str:
        """Generate coding response using OpenAI (optimized for code generation)"""
        try:
            prompt = f"{CODING_PERSONA_PROMPT}\n\n--- Previous Conversation ---\n{context}\n\n--- Final Task ---\n{task}"
            
            # Use OpenAI for coding (best at code generation and VBA)
            return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.3)
            
        except Exception as e:
            raise AIServiceException(f"ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_simple_response(self, question: str) -> str:
        """Generate simple response using Gemini 2.0 Flash (optimized for speed and cost)"""
        try:
            prompt = f"""Excelê³¼ ê´€ë ¨ëœ ê°„ë‹¨í•œ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
1. ê°„ë‹¨í•œ ì„¤ëª…
2. êµ¬ì²´ì ì¸ ì˜ˆì‹œ (í•„ìš”ì‹œ)
3. ì¶”ê°€ íŒ (í•„ìš”ì‹œ)"""
            
            # Use Gemini 2.0 Flash for simple responses (fastest and most cost-effective)
            if self.gemini_flash_model:
                return await self._call_gemini_flash(prompt, temperature=0.3)
            else:
                return await self._call_openai(prompt, model="gpt-4o-mini", temperature=0.3)
            
        except Exception as e:
            raise AIServiceException(f"ê°„ë‹¨í•œ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_creative_response(self, question: str, context: str) -> str:
        """Generate creative response using Gemini 2.0 Pro (optimized for innovative thinking)"""
        try:
            prompt = f"""ì°½ì˜ì ì´ê³  í˜ì‹ ì ì¸ Excel ì†”ë£¨ì…˜ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ìž ì§ˆë¬¸: {question}
ì´ì „ ëŒ€í™”: {context}

ë‹¤ìŒ ê´€ì ì—ì„œ ì ‘ê·¼í•´ì£¼ì„¸ìš”:
1. ìƒˆë¡œìš´ Excel ê¸°ëŠ¥ í™œìš©
2. ìžë™í™” ê°€ëŠ¥ì„±
3. íš¨ìœ¨ì„± ê°œì„  ë°©ì•ˆ
4. ì‚¬ìš©ìž ê²½í—˜ í–¥ìƒ"""
            
            # Use Gemini 2.0 Pro for creative responses (best at innovative thinking)
            if self.gemini_model:
                return await self._call_gemini(prompt, temperature=0.8)
            else:
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.8)
            
        except Exception as e:
            raise AIServiceException(f"ì°½ì˜ì  ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_analytical_response(self, question: str, data_context: str) -> str:
        """Generate analytical response using Gemini 2.0 Pro (optimized for data analysis)"""
        try:
            prompt = f"""ë°ì´í„° ë¶„ì„ ê´€ì ì—ì„œ Excel ìž‘ì—…ì„ ë„ì™€ì£¼ì„¸ìš”.

ë¶„ì„ ìš”ì²­: {question}
ë°ì´í„° ì»¨í…ìŠ¤íŠ¸: {data_context}

ë‹¤ìŒ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”:
1. ë°ì´í„° íŒ¨í„´ ë¶„ì„
2. í†µê³„ì  ì¸ì‚¬ì´íŠ¸
3. ì‹œê°í™” ì œì•ˆ
4. ì¶”ê°€ ë¶„ì„ ë°©í–¥"""
            
            # Use Gemini 2.0 Pro for analytical responses (best at pattern recognition)
            if self.gemini_model:
                return await self._call_gemini(prompt, temperature=0.5)
            else:
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.5)
            
        except Exception as e:
            raise AIServiceException(f"ë¶„ì„ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_debugging_response(self, context: str, feedback: str, image_data: Optional[bytes] = None) -> str:
        """Generate debugging response using OpenAI (optimized for problem-solving)"""
        try:
            # If image is provided, use Gemini 2.0 Flash for image analysis
            image_analysis = ""
            if image_data:
                if self.gemini_flash_model:
                    try:
                        image_analysis = await self._analyze_image_with_gemini(image_data)
                    except Exception as e:
                        image_analysis = f"[ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}]"
                else:
                    image_analysis = "[ì´ë¯¸ì§€ê°€ ì²¨ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. VLOOKUP ì„œì‹ ë¶ˆì¼ì¹˜ ë¬¸ì œë¡œ ì¶”ì •ë©ë‹ˆë‹¤.]"
            
            # Add debugging persona prompt with feedback acknowledgment
            debugging_prompt = f"""ì•„ì´ê³ , ì œê°€ ë“œë¦° ë°©ë²•ì´ í†µí•˜ì§€ ì•Šì•˜êµ°ìš”. ì •ë§ ì£„ì†¡í•´ìš”! ðŸ˜­ ì‚¬ìš©ìžì˜ í”¼ë“œë°±ì„ ìž˜ ë°›ì•˜ê³ , í•¨ê»˜ ë¬¸ì œë¥¼ í•´ê²°í•´ ë³´ë„ë¡ í• ê²Œìš”.

{DEBUGGING_PERSONA_PROMPT}

--- Previous Context ---
{context}

--- User Feedback ---
{feedback}

--- Image Analysis ---
{image_analysis}"""
            
            # Use OpenAI for debugging (best at problem-solving and code analysis)
            return await self._call_openai(debugging_prompt, model=settings.OPENAI_MODEL, temperature=0.5)
            
        except Exception as e:
            raise AIServiceException(f"ë””ë²„ê¹… ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def _call_openai(self, prompt: str, model: str, temperature: float = 0.7) -> str:
        """Make OpenAI API call with enhanced error handling"""
        if not self.openai_client:
            raise AIServiceException("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            response = await asyncio.wait_for(
                self.openai_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=temperature,
                    max_tokens=settings.MAX_TOKENS
                ),
                timeout=settings.AI_REQUEST_TIMEOUT
            )
            
            return response.choices[0].message.content.strip()
            
        except asyncio.TimeoutError:
            raise AIServiceException("OpenAI ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
        except Exception as e:
            raise AIServiceException(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def _call_gemini(self, prompt: str, temperature: float = 0.7) -> str:
        """Make Gemini API call"""
        if not self.gemini_model:
            raise AIServiceException("Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            response = await asyncio.wait_for(
                self.gemini_model.generate_content_async(prompt),
                timeout=settings.AI_REQUEST_TIMEOUT
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            raise AIServiceException("Gemini ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
        except Exception as e:
            raise AIServiceException(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def _call_gemini_flash(self, prompt: str, temperature: float = 0.7) -> str:
        """Make Gemini Flash API call (faster model)"""
        if not self.gemini_flash_model:
            raise AIServiceException("Gemini Flash ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            response = await asyncio.wait_for(
                self.gemini_flash_model.generate_content_async(prompt),
                timeout=30  # Shorter timeout for flash model
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            raise AIServiceException("Gemini Flash ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
        except Exception as e:
            raise AIServiceException(f"Gemini Flash API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def _analyze_image_with_gemini(self, image_data: bytes) -> str:
        """Analyze image using Gemini 2.0 Flash (optimized for image analysis)"""
        # Use Gemini 2.0 Flash for image analysis (fastest and most cost-effective)
        model_to_use = self.gemini_flash_model or self.gemini_model
        
        if not model_to_use:
            return "[ì´ë¯¸ì§€ ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.]"
        
        try:
            image = Image.open(io.BytesIO(image_data))
            
            prompt = """ì´ ì´ë¯¸ì§€ë¥¼ Excel ê´€ë ¨ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. **í™”ë©´ êµ¬ì„±**: ì–´ë–¤ Excel í™”ë©´ì¸ì§€ (ì›Œí¬ì‹œíŠ¸, ì°¨íŠ¸, í”¼ë²— í…Œì´ë¸” ë“±)
2. **ë°ì´í„° êµ¬ì¡°**: í‘œì‹œëœ ë°ì´í„°ì˜ êµ¬ì¡°ì™€ íŠ¹ì§•
3. **ë¬¸ì œì **: ì˜¤ë¥˜ ë©”ì‹œì§€, ìž˜ëª»ëœ ìˆ˜ì‹, ë°ì´í„° ë¶ˆì¼ì¹˜ ë“±
4. **ê°œì„  ë°©ì•ˆ**: ë°œê²¬ëœ ë¬¸ì œì— ëŒ€í•œ í•´ê²°ì±… ì œì•ˆ

íŠ¹ížˆ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ë¶€ë¶„ì´ë‚˜ ë¬¸ì œê°€ ë˜ëŠ” ë¶€ë¶„ì„ ê°•ì¡°í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”."""
            
            response = await asyncio.wait_for(
                model_to_use.generate_content_async([prompt, image]),
                timeout=30
            )
            
            return response.text
            
        except Exception as e:
            return f"[ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}]"
    
    async def process_chat_request(
        self, 
        question: str, 
        context: str, 
        file_summary: str = "",
        is_feedback: bool = False,
        image_data: Optional[bytes] = None,
        conversation_context: Optional[Any] = None
    ) -> AIResponse:
        """Process complete chat request with intelligent model routing and conversation management"""
        start_time = time.time()
        
        try:
            if is_feedback:
                # Handle feedback with debugging persona
                answer = await self.generate_debugging_response(context, question, image_data)
                model_used = settings.OPENAI_MODEL
                response_type = "normal"
                next_action = None
                conversation_state = None
            else:
                # Standard question processing - check if this is a clarification response
                if conversation_context and conversation_context.state.value == "clarifying":
                    # Process clarification response
                    from app.services.conversation_service import conversation_service
                    updated_context = await conversation_service.process_clarification_response(
                        conversation_context, question
                    )
                    
                    if updated_context.state.value == "clarifying":
                        # Still need more clarifications
                        next_question = updated_context.pending_clarifications[0]
                        answer = f"ê°ì‚¬í•©ë‹ˆë‹¤! ë‹¤ìŒ ì§ˆë¬¸ìž…ë‹ˆë‹¤:\n\n**{next_question.question}**\n\n{next_question.context}"
                        model_used = "conversation"
                        response_type = "clarification"
                        next_action = "wait_for_clarification"
                        conversation_state = updated_context.state
                    else:
                        # Ready to generate solution
                        answer = await self._generate_solution_with_context(
                            updated_context.original_question,
                            updated_context.current_understanding,
                            context,
                            file_summary
                        )
                        model_used = settings.OPENAI_MODEL
                        response_type = "solution"
                        next_action = "complete"
                        conversation_state = updated_context.state
                else:
                    # New question - analyze for clarification needs
                    from app.services.conversation_service import conversation_service
                    classification = await conversation_service.analyze_question_for_clarification(question, context)
                    
                    if classification.needs_clarification and not conversation_context:
                        # Start clarification process
                        clarification_type = classification.clarification_reasons[0] if classification.clarification_reasons else "goal"
                        clarification_questions = await conversation_service.generate_clarification_questions(
                            question, clarification_type, context
                        )
                        
                        # Create conversation context
                        conversation_context = conversation_service.create_conversation_context(question)
                        conversation_context.pending_clarifications = clarification_questions
                        conversation_context.state = ConversationState.CLARIFYING
                        
                        # Ask first clarification question
                        first_question = clarification_questions[0]
                        answer = f"ì§ˆë¬¸ì„ ë” ì •í™•í•˜ê²Œ ì´í•´í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤! ðŸ˜Š\n\n**{first_question.question}**\n\n{first_question.context}"
                        model_used = "conversation"
                        response_type = "clarification"
                        next_action = "wait_for_clarification"
                        conversation_state = conversation_context.state
                    else:
                        # Direct answer or continue with existing context
                        if conversation_context and conversation_context.state.value == "planning":
                            # Generate solution with existing context
                            answer = await self._generate_solution_with_context(
                                conversation_context.original_question,
                                conversation_context.current_understanding,
                                context,
                                file_summary
                            )
                            model_used = settings.OPENAI_MODEL
                            response_type = "solution"
                            next_action = "complete"
                            conversation_state = ConversationState.COMPLETED
                        else:
                            # Standard processing with context awareness
                            if conversation_context and conversation_context.state.value in ["planning", "executing"]:
                                # Continue with existing conversation context
                                answer = await self._generate_solution_with_context(
                                    conversation_context.original_question,
                                    conversation_context.current_understanding or question,
                                    context,
                                    file_summary
                                )
                                model_used = settings.OPENAI_MODEL
                                response_type = "solution"
                                next_action = "complete"
                                conversation_state = ConversationState.COMPLETED
                            else:
                                # Standard processing for new questions
                                answer = await self._process_standard_question(question, context, file_summary, classification, image_data)
                                model_used = classification.recommended_model or settings.OPENAI_MODEL
                                response_type = "normal"
                                next_action = None
                                conversation_state = None
            
            processing_time = time.time() - start_time
            
            return AIResponse(
                answer=answer,
                session_id="",  # Will be set by caller
                model_used=model_used,
                processing_time=processing_time,
                response_type=response_type,
                next_action=next_action,
                conversation_state=conversation_state
            )
            
        except Exception as e:
            raise AIServiceException(f"ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    async def _process_standard_question(
        self, 
        question: str, 
        context: str, 
        file_summary: str,
        classification: QuestionClassification,
        image_data: Optional[bytes] = None
    ) -> str:
        """Process standard question without clarification needs"""
        if classification.classification == "simple":
            return await self.generate_simple_response(question)
        elif classification.classification == "creative":
            return await self.generate_creative_response(question, context)
        elif classification.classification == "analytical":
            return await self.generate_analytical_response(question, file_summary)
        elif classification.classification == "debugging":
            return await self.generate_debugging_response(context, question, image_data)
        else:
            # Complex question - generate planning response
            return await self.generate_planning_response(context, file_summary)
    
    async def _generate_solution_with_context(
        self,
        original_question: str,
        understanding: str,
        context: str,
        file_summary: str
    ) -> str:
        """Generate solution based on gathered context and understanding"""
        try:
            # Determine if this is a continuation or new solution
            is_continuation = len(context.strip()) > 100  # If there's substantial context
            
            if is_continuation:
                prompt = f"""ì´ì „ ëŒ€í™”ë¥¼ ì´ì–´ì„œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.

ì›ëž˜ ì§ˆë¬¸: "{original_question}"
í˜„ìž¬ ìƒí™©: {understanding}

ì´ì „ ëŒ€í™” ë‚´ìš©:
{context}

íŒŒì¼ ì •ë³´: {file_summary}

ì´ì „ ëŒ€í™”ë¥¼ ê³ ë ¤í•˜ì—¬ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

1. **ì´ì „ ë‹µë³€ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ë³´ì™„**ì´ í•„ìš”í•œ ê²½ìš°
2. **ìƒˆë¡œìš´ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­**ì´ ìžˆëŠ” ê²½ìš°
3. **ì´ì „ í•´ê²°ì±…ì˜ ì‹¤í–‰ ê²°ê³¼**ì— ëŒ€í•œ í”¼ë“œë°±ì´ ìžˆëŠ” ê²½ìš°

ìžì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ë©° ì‚¬ìš©ìžì˜ ìš”êµ¬ì— ë§žëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""
            else:
                prompt = f"""ì‚¬ìš©ìžì˜ ì§ˆë¬¸: "{original_question}"

ì´í•´í•œ ë‚´ìš©: {understanding}

ì´ì „ ëŒ€í™”: {context}

íŒŒì¼ ì •ë³´: {file_summary}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì™„ë²½í•œ í•´ê²°ì±…ì„ ì œì‹œí•´ì£¼ì„¸ìš”. ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”:

## ðŸŽ¯ í•´ê²°ì±…

[ë¬¸ì œ ìš”ì•½]
- ì‚¬ìš©ìžê°€ í•´ê²°í•˜ê³ ìž í•˜ëŠ” ë¬¸ì œ

[í•´ê²° ë°©ë²•]
- êµ¬ì²´ì ì¸ í•´ê²° ë°©ë²• (VBA ì½”ë“œ, í•¨ìˆ˜, ìˆ˜ì‹ ë“±)

[ì‚¬ìš© ë°©ë²•]
- ë‹¨ê³„ë³„ ì‚¬ìš© ë°©ë²•

[ì£¼ì˜ì‚¬í•­]
- ì£¼ì˜í•´ì•¼ í•  ì ë“¤

[ì¶”ê°€ íŒ]
- ë” ë‚˜ì€ ë°©ë²•ì´ë‚˜ ê°œì„  ë°©ì•ˆ"""

            # Use OpenAI for solution generation
            return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.3)
            
        except Exception as e:
            raise AIServiceException(f"í•´ê²°ì±… ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    def get_service_status(self) -> Dict[str, Any]:
        """Get comprehensive AI service status with optimized model information"""
        return {
            "openai_available": self.openai_client is not None,
            "gemini_available": self.gemini_model is not None,
            "gemini_flash_available": self.gemini_flash_model is not None,
            "models": {
                "openai": settings.OPENAI_MODEL,
                "gemini": settings.GEMINI_PRO_MODEL,
                "gemini_flash": settings.GEMINI_FLASH_MODEL
            },
            "capabilities": {
                "text_generation": True,
                "image_analysis": self.gemini_flash_model is not None,
                "fast_processing": self.gemini_flash_model is not None,
                "creative_thinking": self.gemini_model is not None,
                "code_generation": self.openai_client is not None,
                "data_analysis": self.gemini_model is not None,
                "planning": self.gemini_model is not None
            },
            "optimization": {
                "classification": "Gemini 2.0 Flash (fastest)",
                "simple_questions": "Gemini 2.0 Flash (cost-effective)",
                "creative_tasks": "Gemini 2.0 Pro (innovative)",
                "analytical_tasks": "Gemini 2.0 Pro (pattern recognition)",
                "coding_tasks": "OpenAI GPT-4o-mini (best code generation)",
                "debugging": "OpenAI GPT-4o-mini (problem-solving)",
                "image_analysis": "Gemini 2.0 Flash (multimodal)",
                "planning": "Gemini 2.0 Pro (structured thinking)"
            }
        }

# Global AI service instance
ai_service = AIService()
