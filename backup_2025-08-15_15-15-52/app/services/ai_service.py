# app/services/ai_service.py
# AI model management service

import time
import asyncio
import json
from typing import Optional, Dict, Any, List, Tuple
from openai import AsyncOpenAI
import google.generativeai as genai
from PIL import Image
import io

from app.core.config import settings
from app.core.exceptions import AIServiceException
from app.models.chat import QuestionClassification, UserIntent, AIResponse, ConversationState
from prompts import PLANNING_PERSONA_PROMPT, CODING_PERSONA_PROMPT, DEBUGGING_PERSONA_PROMPT

class AIService:
    """Service for managing AI model interactions with intelligent routing"""
    
    def __init__(self):
        self.openai_client = None
        self.gemini_model = None
        self.gemini_flash_model = None
        self._initialize_clients()
    
    def _initialize_clients(self):
        """Initialize AI clients with optimized model selection"""
        try:
            # Initialize OpenAI client
            if settings.OPENAI_API_KEY:
                self.openai_client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
                print(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ: {settings.OPENAI_MODEL}")
            
            # Initialize Gemini client
            if settings.GEMINI_API_KEY:
                genai.configure(api_key=settings.GEMINI_API_KEY)
                
                # Initialize Gemini Pro model (for creative and analytical tasks)
                try:
                    self.gemini_model = genai.GenerativeModel(settings.GEMINI_PRO_MODEL)
                    print(f"âœ… Gemini Pro ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ: {settings.GEMINI_PRO_MODEL}")
                except Exception as e:
                    print(f"âš ï¸ Gemini Pro ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    # Fallback to 2.0 Pro, then 1.5 Pro
                    try:
                        self.gemini_model = genai.GenerativeModel(settings.GEMINI_2_0_PRO_FALLBACK)
                        print(f"âœ… Gemini 2.0 Pro ëª¨ë¸ë¡œ ëŒ€ì²´: {settings.GEMINI_2_0_PRO_FALLBACK}")
                    except Exception as e2:
                        print(f"âš ï¸ Gemini 2.0 Pro ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e2}")
                        try:
                            self.gemini_model = genai.GenerativeModel(settings.GEMINI_1_5_PRO_FALLBACK)
                            print(f"âœ… Gemini 1.5 Pro ëª¨ë¸ë¡œ ëŒ€ì²´: {settings.GEMINI_1_5_PRO_FALLBACK}")
                        except Exception as e3:
                            print(f"âŒ Gemini Pro ê³„ì—´ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e3}")
                            self.gemini_model = None
                
                # Initialize Gemini Flash model (for fast processing and classification)
                try:
                    self.gemini_flash_model = genai.GenerativeModel(settings.GEMINI_FLASH_MODEL)
                    print(f"âœ… Gemini Flash ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ: {settings.GEMINI_FLASH_MODEL}")
                except Exception as e:
                    print(f"âš ï¸ Gemini Flash ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    # Fallback to 2.0 Flash, then 1.5 Flash
                    try:
                        self.gemini_flash_model = genai.GenerativeModel(settings.GEMINI_2_0_FLASH_FALLBACK)
                        print(f"âœ… Gemini 2.0 Flash ëª¨ë¸ë¡œ ëŒ€ì²´: {settings.GEMINI_2_0_FLASH_FALLBACK}")
                    except Exception as e2:
                        print(f"âš ï¸ Gemini 2.0 Flash ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e2}")
                        try:
                            self.gemini_flash_model = genai.GenerativeModel(settings.GEMINI_1_5_FLASH_FALLBACK)
                            print(f"âœ… Gemini 1.5 Flash ëª¨ë¸ë¡œ ëŒ€ì²´: {settings.GEMINI_1_5_FLASH_FALLBACK}")
                        except Exception as e3:
                            print(f"âŒ Gemini Flash ê³„ì—´ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e3}")
                            self.gemini_flash_model = None
                
        except Exception as e:
            print(f"âš ï¸ AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # Don't raise exception, continue with available models
    
    async def classify_question(self, question: str) -> QuestionClassification:
        """Enhanced question classification with optimized model selection"""
        try:
            prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ AI ëª¨ë¸ê³¼ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •í•˜ì„¸ìš”.

ì§ˆë¬¸: "{question}"

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
1. **simple**: ê°„ë‹¨í•œ ì‚¬ì‹¤ ì§ˆë¬¸, í•¨ìˆ˜ ì‚¬ìš©ë²•, ê¸°ë³¸ ë¬¸ë²•
2. **complex**: ë³µì¡í•œ ë¡œì§, ì—¬ëŸ¬ ë‹¨ê³„ ì‘ì—…, íŒŒì¼ ë¶„ì„
3. **creative**: ì°½ì˜ì  ì•„ì´ë””ì–´, ìƒˆë¡œìš´ ì ‘ê·¼ë²•, ìµœì í™” ì œì•ˆ
4. **analytical**: ë°ì´í„° ë¶„ì„, íŒ¨í„´ ì°¾ê¸°, í†µê³„ì  ë¶„ì„
5. **debugging**: ì˜¤ë¥˜ í•´ê²°, ë¬¸ì œ ì§„ë‹¨, ì½”ë“œ ìˆ˜ì •

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”:
{{
    "classification": "simple|complex|creative|analytical|debugging",
    "confidence": 0.0-1.0,
    "reasoning": "ë¶„ë¥˜ ì´ìœ ",
    "recommended_model": "openai|gemini_pro|gemini_flash",
    "estimated_tokens": 100-4000
}}"""
            
            # Use Gemini 2.0 Flash for classification (fastest and most cost-effective)
            if self.gemini_flash_model:
                response = await self._call_gemini_flash(prompt, temperature=0.0)
            else:
                response = await self._call_openai(prompt, model="gpt-4o-mini", temperature=0.0)
            
            try:
                result = json.loads(response)
                return QuestionClassification(
                    classification=result.get("classification", "complex"),
                    confidence=result.get("confidence", 0.5),
                    reasoning=result.get("reasoning", ""),
                    recommended_model=result.get("recommended_model", "openai"),
                    estimated_tokens=result.get("estimated_tokens", 1000)
                )
            except json.JSONDecodeError:
                # Fallback parsing
                classification = self._fallback_classification(response)
                return QuestionClassification(
                    classification=classification,
                    confidence=0.7,
                    reasoning="JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ë¶„ë¥˜ ì‚¬ìš©",
                    recommended_model="openai",
                    estimated_tokens=1000
                )
                
        except Exception as e:
            return QuestionClassification(
                classification="complex",
                confidence=0.5,
                reasoning=f"ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}",
                recommended_model="openai",
                estimated_tokens=1000
            )
    
    def _fallback_classification(self, response: str) -> str:
        """Fallback classification when JSON parsing fails"""
        response_lower = response.lower()
        if any(word in response_lower for word in ["simple", "ê°„ë‹¨", "ê¸°ë³¸"]):
            return "simple"
        elif any(word in response_lower for word in ["creative", "ì°½ì˜", "ìƒˆë¡œìš´"]):
            return "creative"
        elif any(word in response_lower for word in ["analytical", "ë¶„ì„", "í†µê³„"]):
            return "analytical"
        elif any(word in response_lower for word in ["debugging", "ì˜¤ë¥˜", "ë¬¸ì œ"]):
            return "debugging"
        else:
            return "complex"
    
    async def analyze_user_intent(self, plan: str, user_reply: str) -> UserIntent:
        """Analyze user's intent using Gemini 2.0 Flash (optimized for fast analysis)"""
        try:
            prompt = f"""AIì˜ ê³„íš: "{plan[:200]}..."
ì‚¬ìš©ì ë‹µë³€: "{user_reply}"

ì‚¬ìš©ì ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”:
- "agreement": ê³„íšì— ë™ì˜í•˜ê³  ì§„í–‰
- "modification": ê³„íš ìˆ˜ì • ìš”ì²­
- "clarification": ì¶”ê°€ ì„¤ëª… ìš”ì²­
- "rejection": ê³„íš ê±°ë¶€

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”:
{{
    "intent": "agreement|modification|clarification|rejection",
    "confidence": 0.0-1.0,
    "reasoning": "ì˜ë„ ë¶„ì„ ì´ìœ "
}}"""
            
            # Use Gemini 2.0 Flash for intent analysis (fastest and most cost-effective)
            if self.gemini_flash_model:
                response = await self._call_gemini_flash(prompt, temperature=0.0)
            else:
                response = await self._call_openai(prompt, model="gpt-4o-mini", temperature=0.0)
            
            try:
                result = json.loads(response)
                return UserIntent(
                    intent=result.get("intent", "other"),
                    confidence=result.get("confidence", 0.5),
                    reasoning=result.get("reasoning", "")
                )
            except json.JSONDecodeError:
                intent = self._fallback_intent_analysis(response)
                return UserIntent(
                    intent=intent,
                    confidence=0.7,
                    reasoning="JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ë¶„ì„ ì‚¬ìš©"
                )
                
        except Exception as e:
            return UserIntent(
                intent="other",
                confidence=0.5,
                reasoning=f"ì˜ë„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            )
    
    def _fallback_intent_analysis(self, response: str) -> str:
        """Fallback intent analysis when JSON parsing fails"""
        response_lower = response.lower()
        if any(word in response_lower for word in ["agreement", "ë™ì˜", "ì¢‹ë‹¤", "ì§„í–‰"]):
            return "agreement"
        elif any(word in response_lower for word in ["modification", "ìˆ˜ì •", "ë³€ê²½"]):
            return "modification"
        elif any(word in response_lower for word in ["clarification", "ì„¤ëª…", "ì´í•´"]):
            return "clarification"
        elif any(word in response_lower for word in ["rejection", "ê±°ë¶€", "ì•„ë‹ˆ"]):
            return "rejection"
        else:
            return "other"
    
    async def generate_planning_response(self, context: str, file_summary: str = "", answer_style: Optional[str] = None) -> str:
        """Generate planning response using Gemini 2.0 Pro (optimized for structured thinking)"""
        try:
            style_guard = "\n\n[ì‘ë‹µ ìŠ¤íƒ€ì¼] í•µì‹¬ë§Œ ê°„ê²°íˆ 5ì¤„ ì´ë‚´ë¡œ ìš”ì•½" if (answer_style=="concise") else ""
            prompt = f"{PLANNING_PERSONA_PROMPT}{style_guard}\n\n--- Conversation History ---\n{context}\n\n{file_summary}"
            
            # Use Gemini 2.0 Pro for planning (best at structured thinking and planning)
            if self.gemini_model:
                return await self._call_gemini(prompt, temperature=0.7)
            else:
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.7)
            
        except Exception as e:
            raise AIServiceException(f"ê³„íš ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_coding_response(self, context: str, task: str) -> str:
        """Generate coding response using OpenAI (optimized for code generation)"""
        try:
            prompt = f"{CODING_PERSONA_PROMPT}\n\n--- Previous Conversation ---\n{context}\n\n--- Final Task ---\n{task}"
            
            # Use OpenAI for coding (best at code generation and VBA)
            return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.3)
            
        except Exception as e:
            raise AIServiceException(f"ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_simple_response(self, question: str, answer_style: Optional[str] = None) -> str:
        """Generate simple response using Gemini 2.0 Flash (optimized for speed and cost)"""
        try:
            prompt = f"""Excelê³¼ ê´€ë ¨ëœ ê°„ë‹¨í•œ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
1. ê°„ë‹¨í•œ ì„¤ëª…
2. êµ¬ì²´ì ì¸ ì˜ˆì‹œ (í•„ìš”ì‹œ)
3. ì¶”ê°€ íŒ (í•„ìš”ì‹œ)"""
            if answer_style == "concise":
                prompt += "\n\n[ì‘ë‹µ ìŠ¤íƒ€ì¼] í•µì‹¬ë§Œ ê°„ê²°íˆ, 5ì¤„ ì´ë‚´"
            
            # Use Gemini 2.0 Flash for simple responses (fastest and most cost-effective)
            if self.gemini_flash_model:
                return await self._call_gemini_flash(prompt, temperature=0.3)
            else:
                return await self._call_openai(prompt, model="gpt-4o-mini", temperature=0.3)
            
        except Exception as e:
            raise AIServiceException(f"ê°„ë‹¨í•œ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_creative_response(self, question: str, context: str, answer_style: Optional[str] = None) -> str:
        """Generate creative response using Gemini 2.0 Pro (optimized for innovative thinking)"""
        try:
            prompt = f"""ì°½ì˜ì ì´ê³  í˜ì‹ ì ì¸ Excel ì†”ë£¨ì…˜ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {question}
ì´ì „ ëŒ€í™”: {context}

ë‹¤ìŒ ê´€ì ì—ì„œ ì ‘ê·¼í•´ì£¼ì„¸ìš”:
1. ìƒˆë¡œìš´ Excel ê¸°ëŠ¥ í™œìš©
2. ìë™í™” ê°€ëŠ¥ì„±
3. íš¨ìœ¨ì„± ê°œì„  ë°©ì•ˆ
4. ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ"""
            if answer_style == "concise":
                prompt += "\n\n[ì‘ë‹µ ìŠ¤íƒ€ì¼] í•µì‹¬ë§Œ ê°„ê²°íˆ, 5ì¤„ ì´ë‚´"
            # Use Gemini 2.0 Pro for creative responses (best at innovative thinking)
            if self.gemini_model:
                return await self._call_gemini(prompt, temperature=0.8)
            else:
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.8)
            
        except Exception as e:
            raise AIServiceException(f"ì°½ì˜ì  ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_analytical_response(self, question: str, data_context: str, answer_style: Optional[str] = None) -> str:
        """Generate analytical response using Gemini 2.0 Pro (optimized for data analysis)"""
        try:
            prompt = f"""ë°ì´í„° ë¶„ì„ ê´€ì ì—ì„œ Excel ì‘ì—…ì„ ë„ì™€ì£¼ì„¸ìš”.

ë¶„ì„ ìš”ì²­: {question}
ë°ì´í„° ì»¨í…ìŠ¤íŠ¸: {data_context}

ë‹¤ìŒ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”:
1. ë°ì´í„° íŒ¨í„´ ë¶„ì„
2. í†µê³„ì  ì¸ì‚¬ì´íŠ¸
3. ì‹œê°í™” ì œì•ˆ
4. ì¶”ê°€ ë¶„ì„ ë°©í–¥"""
            if answer_style == "concise":
                prompt += "\n\n[ì‘ë‹µ ìŠ¤íƒ€ì¼] í•µì‹¬ë§Œ ê°„ê²°íˆ, 5ì¤„ ì´ë‚´"
            # Use Gemini 2.0 Pro for analytical responses (best at pattern recognition)
            if self.gemini_model:
                return await self._call_gemini(prompt, temperature=0.5)
            else:
                return await self._call_openai(prompt, model=settings.OPENAI_MODEL, temperature=0.5)
            
        except Exception as e:
            raise AIServiceException(f"ë¶„ì„ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def generate_debugging_response(self, context: str, feedback: str, image_data: Optional[bytes] = None) -> str:
        """Generate debugging response using OpenAI (optimized for problem-solving)"""
        try:
            # If image is provided, use Gemini 2.0 Flash for image analysis
            image_analysis = ""
            if image_data:
                if self.gemini_flash_model:
                    try:
                        image_analysis = await self._analyze_image_with_gemini(image_data)
                    except Exception as e:
                        image_analysis = f"[ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}]"
                else:
                    image_analysis = "[ì´ë¯¸ì§€ê°€ ì²¨ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. VLOOKUP ì„œì‹ ë¶ˆì¼ì¹˜ ë¬¸ì œë¡œ ì¶”ì •ë©ë‹ˆë‹¤.]"
            
            # Add debugging persona prompt with feedback acknowledgment
            debugging_prompt = f"""ì•„ì´ê³ , ì œê°€ ë“œë¦° ë°©ë²•ì´ í†µí•˜ì§€ ì•Šì•˜êµ°ìš”. ì •ë§ ì£„ì†¡í•´ìš”! ğŸ˜­ ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ì˜ ë°›ì•˜ê³ , í•¨ê»˜ ë¬¸ì œë¥¼ í•´ê²°í•´ ë³´ë„ë¡ í• ê²Œìš”.

{DEBUGGING_PERSONA_PROMPT}

--- Previous Context ---
{context}

--- User Feedback ---
{feedback}

--- Image Analysis ---
{image_analysis}"""
            
            # Use OpenAI for debugging (best at problem-solving and code analysis)
            return await self._call_openai(debugging_prompt, model=settings.OPENAI_MODEL, temperature=0.5)
            
        except Exception as e:
            raise AIServiceException(f"ë””ë²„ê¹… ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def _call_openai(self, prompt: str, model: str, temperature: float = 0.7) -> str:
        """Make OpenAI API call with enhanced error handling"""
        if not self.openai_client:
            raise AIServiceException("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            response = await asyncio.wait_for(
                self.openai_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=temperature,
                    max_tokens=settings.MAX_TOKENS
                ),
                timeout=settings.AI_REQUEST_TIMEOUT
            )
            
            return response.choices[0].message.content.strip()
            
        except asyncio.TimeoutError:
            raise AIServiceException("OpenAI ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
        except Exception as e:
            raise AIServiceException(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def _call_gemini(self, prompt: str, temperature: float = 0.7) -> str:
        """Make Gemini API call"""
        if not self.gemini_model:
            raise AIServiceException("Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            response = await asyncio.wait_for(
                self.gemini_model.generate_content_async(prompt),
                timeout=settings.AI_REQUEST_TIMEOUT
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            raise AIServiceException("Gemini ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
        except Exception as e:
            raise AIServiceException(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def _call_gemini_flash(self, prompt: str, temperature: float = 0.7) -> str:
        """Make Gemini Flash API call (faster model)"""
        if not self.gemini_flash_model:
            raise AIServiceException("Gemini Flash ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            response = await asyncio.wait_for(
                self.gemini_flash_model.generate_content_async(prompt),
                timeout=30  # Shorter timeout for flash model
            )
            
            return response.text.strip()
            
        except asyncio.TimeoutError:
            # Fallback to OpenAI on timeout
            try:
                return await self._call_openai(prompt, model="gpt-4o-mini", temperature=temperature)
            except Exception as e2:
                raise AIServiceException(f"Gemini Flash ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ ë° OpenAI í´ë°± ì‹¤íŒ¨: {str(e2)}")
        except Exception as e:
            # Fallback to OpenAI on general error
            try:
                return await self._call_openai(prompt, model="gpt-4o-mini", temperature=temperature)
            except Exception as e2:
                raise AIServiceException(f"Gemini Flash API ì‹¤íŒ¨ ë° OpenAI í´ë°± ì‹¤íŒ¨: {str(e2)}")
    
    async def _analyze_image_with_gemini(self, image_data: bytes) -> str:
        """Analyze image using Gemini 2.0 Flash (optimized for image analysis)"""
        # Use Gemini 2.0 Flash for image analysis (fastest and most cost-effective)
        model_to_use = self.gemini_flash_model or self.gemini_model
        
        if not model_to_use:
            return "[ì´ë¯¸ì§€ ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.]"
        
        try:
            image = Image.open(io.BytesIO(image_data))
            
            prompt = """ì´ ì´ë¯¸ì§€ë¥¼ Excel ê´€ë ¨ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. **í™”ë©´ êµ¬ì„±**: ì–´ë–¤ Excel í™”ë©´ì¸ì§€ (ì›Œí¬ì‹œíŠ¸, ì°¨íŠ¸, í”¼ë²— í…Œì´ë¸” ë“±)
2. **ë°ì´í„° êµ¬ì¡°**: í‘œì‹œëœ ë°ì´í„°ì˜ êµ¬ì¡°ì™€ íŠ¹ì§•
3. **ë¬¸ì œì **: ì˜¤ë¥˜ ë©”ì‹œì§€, ì˜ëª»ëœ ìˆ˜ì‹, ë°ì´í„° ë¶ˆì¼ì¹˜ ë“±
4. **ê°œì„  ë°©ì•ˆ**: ë°œê²¬ëœ ë¬¸ì œì— ëŒ€í•œ í•´ê²°ì±… ì œì•ˆ

íŠ¹íˆ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ë¶€ë¶„ì´ë‚˜ ë¬¸ì œê°€ ë˜ëŠ” ë¶€ë¶„ì„ ê°•ì¡°í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”."""
            
            response = await asyncio.wait_for(
                model_to_use.generate_content_async([prompt, image]),
                timeout=30
            )
            
            return response.text
            
        except Exception as e:
            return f"[ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}]"
    
    async def process_chat_request(
        self, 
        question: str, 
        context: str, 
        file_summary: str = "",
        is_feedback: bool = False,
        image_data: Optional[bytes] = None,
        conversation_context: Optional[Any] = None,
        answer_style: Optional[str] = None
    ) -> AIResponse:
        """Process complete chat request with intelligent model routing and conversation management"""
        start_time = time.time()
        
        try:
            if is_feedback:
                # Handle feedback with debugging persona
                answer = await self.generate_debugging_response(context, question, image_data)
                model_used = settings.OPENAI_MODEL
                response_type = "normal"
                next_action = None
                conversation_state = None
            else:
                # Special flow: if a sheet was selected and we have file summary, provide detailed analysis
                # ONLY if question is empty, starts with [ì‹œíŠ¸ì„ íƒ], or is very short (like "1", "2", etc.)
                if (file_summary and file_summary.strip()) and (
                    (not question) or 
                    question.strip().startswith("[ì‹œíŠ¸ì„ íƒ]") or
                    (len(question.strip()) <= 2 and question.strip().isdigit())
                ):
                    # Extract sheet name from file summary
                    sheet_name = "ì„ íƒëœ ì‹œíŠ¸"
                    if "ì„ íƒëœ ì‹œíŠ¸: '" in file_summary:
                        start_idx = file_summary.find("ì„ íƒëœ ì‹œíŠ¸: '") + len("ì„ íƒëœ ì‹œíŠ¸: '")
                        end_idx = file_summary.find("'", start_idx)
                        if end_idx > start_idx:
                            sheet_name = file_summary[start_idx:end_idx]
                    
                    answer = f"""ğŸ“Š **{sheet_name} ì‹œíŠ¸ ë¶„ì„ ì™„ë£Œ!**

{file_summary}

---

ğŸ¯ **ì´ì œ ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?**

1ï¸âƒ£ **ìˆ˜ì‹/í•¨ìˆ˜ ë§Œë“¤ê¸°** - VLOOKUP, SUMIF, INDEX/MATCH ë“±
2ï¸âƒ£ **ë°ì´í„° ì •ë¦¬** - ì¤‘ë³µ ì œê±°, ì •ë ¬, í•„í„°ë§
3ï¸âƒ£ **ìš”ì•½/ë¶„ì„** - í”¼ë²— í…Œì´ë¸”, í†µê³„ ë¶„ì„
4ï¸âƒ£ **ì‹œê°í™”** - ì°¨íŠ¸, ê·¸ë˜í”„ ë§Œë“¤ê¸°
5ï¸âƒ£ **ìë™í™”** - VBA ë§¤í¬ë¡œ, ë°˜ë³µ ì‘ì—… ìë™í™”

ë²ˆí˜¸ë¡œ ë‹µí•˜ì‹œê±°ë‚˜, êµ¬ì²´ì ìœ¼ë¡œ ì›í•˜ì‹œëŠ” ì‘ì—…ì„ ë§ì”€í•´ ì£¼ì„¸ìš”!
ì˜ˆ: "Aì—´ì˜ ì¤‘ë³µì„ ì œê±°í•˜ê³  Bì—´ë¡œ ì •ë ¬í•´ì¤˜" ë˜ëŠ” "ë§¤ì¶œ ë°ì´í„°ë¡œ í”¼ë²— í…Œì´ë¸” ë§Œë“¤ì–´ì¤˜"
"""
                    model_used = "conversation"
                    response_type = "clarification"
                    next_action = "wait_for_clarification"
                    conversation_state = ConversationState.CLARIFYING
                    # Return immediately to avoid being overwritten by later branches
                    processing_time = time.time() - start_time
                    return AIResponse(
                        answer=answer,
                        session_id="",
                        model_used=model_used,
                        processing_time=processing_time,
                        response_type=response_type,
                        next_action=next_action,
                        conversation_state=conversation_state
                    )
                # Check if user provided a specific task request (contains function names, specific operations)
                elif file_summary and file_summary.strip() and self._is_specific_task_request(question):
                    # User provided a specific task - process directly
                    answer = await self._generate_solution_with_context(
                        question,  # Use the question as the task
                        question,  # Use the question as understanding
                        context,
                        file_summary
                    )
                    model_used = settings.OPENAI_MODEL
                    response_type = "solution"
                    next_action = "complete"
                    conversation_state = ConversationState.COMPLETED
                # Check if user mentioned VBA or specific complex operations
                elif file_summary and file_summary.strip() and self._is_vba_or_complex_request(question):
                    # User requested VBA or complex operations - process directly
                    answer = await self._generate_solution_with_context(
                        question,  # Use the question as the task
                        question,  # Use the question as understanding
                        context,
                        file_summary
                    )
                    model_used = settings.OPENAI_MODEL
                    response_type = "solution"
                    next_action = "complete"
                    conversation_state = ConversationState.COMPLETED
                # Check if user selected a task option (1-5)
                elif file_summary and file_summary.strip() and question.strip() in ["1", "2", "3", "4", "5"]:
                    # User selected a task option
                    task_map = {
                        "1": "ìˆ˜ì‹/í•¨ìˆ˜ ë§Œë“¤ê¸°",
                        "2": "ë°ì´í„° ì •ë¦¬", 
                        "3": "ìš”ì•½/ë¶„ì„",
                        "4": "ì‹œê°í™”",
                        "5": "ìë™í™”"
                    }
                    selected_task = task_map.get(question.strip(), "ì¼ë°˜ ì‘ì—…")
                    
                    answer = f"""âœ… **{selected_task}ë¥¼ ì„ íƒí•˜ì…¨ìŠµë‹ˆë‹¤!**

í˜„ì¬ íŒŒì¼ ì •ë³´:
{file_summary}

êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì‘ì—…ì„ ì›í•˜ì‹œë‚˜ìš”?

**{selected_task} ì˜ˆì‹œ:**
{self._get_task_examples(selected_task)}

ì›í•˜ì‹œëŠ” ì‘ì—…ì„ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”!
ì˜ˆ: "VLOOKUPìœ¼ë¡œ ë‹¤ë¥¸ ì‹œíŠ¸ì™€ ì—°ê²°í•´ì¤˜" ë˜ëŠ” "Aì—´ ì¤‘ë³µ ì œê±°í•´ì¤˜"
"""
                    model_used = "conversation"
                    response_type = "task_selection"
                    next_action = "wait_for_task_details"
                    conversation_state = ConversationState.CLARIFYING
                # Standard question processing - check if this is a clarification response
                elif conversation_context and conversation_context.state.value == "clarifying":
                    # Process clarification response
                    from app.services.conversation_service import conversation_service
                    updated_context = await conversation_service.process_clarification_response(
                        conversation_context, question
                    )
                    
                    if updated_context.state.value == "clarifying":
                        # Still need more clarifications
                        next_question = updated_context.pending_clarifications[0]
                        answer = f"ê°ì‚¬í•©ë‹ˆë‹¤! ë‹¤ìŒ ì§ˆë¬¸ì…ë‹ˆë‹¤:\n\n**{next_question.question}**\n\n{next_question.context}"
                        model_used = "conversation"
                        response_type = "clarification"
                        next_action = "wait_for_clarification"
                        conversation_state = updated_context.state
                    else:
                        # Ready to generate solution
                        answer = await self._generate_solution_with_context(
                            updated_context.original_question,
                            updated_context.current_understanding,
                            context,
                            file_summary
                        )
                        model_used = settings.OPENAI_MODEL
                        response_type = "solution"
                        next_action = "complete"
                        conversation_state = updated_context.state
                else:
                    # New question - analyze for clarification needs
                    from app.services.conversation_service import conversation_service
                    
                    # STRICT clarification logic - only clarify when absolutely necessary
                    needs_clarification = False
                    clarification_reasons = []
                    
                    # Case 1: Empty or very short question with file
                    if (file_summary and file_summary.strip()) and (not question or len(question.strip()) < 2):
                        needs_clarification = True
                        clarification_reasons = ["goal"]
                    # Case 2: Question is too vague (no specific function, operation, or target mentioned)
                    elif not self._is_specific_enough(question):
                        needs_clarification = True
                        clarification_reasons = ["goal"]
                    # Case 3: Question mentions multiple possible interpretations
                    elif self._has_multiple_interpretations(question):
                        needs_clarification = True
                        clarification_reasons = ["goal"]
                    
                    # Create classification based on strict rules
                    if needs_clarification:
                        classification = QuestionClassification(
                            classification="complex",
                            confidence=0.9,
                            reasoning="êµ¬ì²´ì ì¸ ìš”ì²­ì´ ë¶€ì¡±í•˜ì—¬ ëª…í™•í™” í•„ìš”",
                            recommended_model="openai",
                            estimated_tokens=300,
                            needs_clarification=True,
                            clarification_reasons=clarification_reasons
                        )
                    else:
                        classification = QuestionClassification(
                            classification="complex",
                            confidence=0.8,
                            reasoning="êµ¬ì²´ì ì¸ ìš”ì²­ìœ¼ë¡œ ë°”ë¡œ ì²˜ë¦¬ ê°€ëŠ¥",
                            recommended_model="openai",
                            estimated_tokens=300,
                            needs_clarification=False,
                            clarification_reasons=[]
                        )
                    
                    # Only clarify if absolutely necessary
                    if classification.needs_clarification and not conversation_context:
                        # Start clarification process
                        clarification_type = classification.clarification_reasons[0] if classification.clarification_reasons else "goal"
                        clarification_questions = await conversation_service.generate_clarification_questions(
                            question, clarification_type, context
                        )
                        
                        # Create conversation context
                        conversation_context = conversation_service.create_conversation_context(question)
                        conversation_context.pending_clarifications = clarification_questions
                        conversation_context.state = ConversationState.CLARIFYING
                        
                        # Ask first clarification question (very short & user-friendly)
                        first_question = clarification_questions[0]
                        answer = f"ì¢‹ì•„ìš”! ì •í™•íˆ ë„ì™€ë“œë¦¬ë ¤ë©´ í•œ ê°€ì§€ë§Œ ì•Œë ¤ì£¼ì„¸ìš”.\n\n**{first_question.question}**\n\n{first_question.context}"
                        model_used = "conversation"
                        response_type = "clarification"
                        next_action = "wait_for_clarification"
                        conversation_state = conversation_context.state
                    else:
                        # Direct answer or continue with existing context
                        if conversation_context and conversation_context.state.value == "planning":
                            # Generate solution with existing context
                            answer = await self._generate_solution_with_context(
                                conversation_context.original_question,
                                conversation_context.current_understanding,
                                context,
                                file_summary
                            )
                            model_used = settings.OPENAI_MODEL
                            response_type = "solution"
                            next_action = "complete"
                            conversation_state = ConversationState.COMPLETED
                        else:
                            # Standard processing with context awareness
                            if conversation_context and conversation_context.state.value in ["planning", "executing"]:
                                # Continue with existing conversation context
                                answer = await self._generate_solution_with_context(
                                    conversation_context.original_question,
                                    conversation_context.current_understanding or question,
                                    context,
                                    file_summary
                                )
                                model_used = settings.OPENAI_MODEL
                                response_type = "solution"
                                next_action = "complete"
                                conversation_state = ConversationState.COMPLETED
                            else:
                                # Standard processing for new questions
                                answer = await self._process_standard_question(question, context, file_summary, classification, image_data, answer_style)
                                model_used = classification.recommended_model or settings.OPENAI_MODEL
                                response_type = "normal"
                                next_action = None
                                conversation_state = None
            
            processing_time = time.time() - start_time
            
            return AIResponse(
                answer=answer,
                session_id="",  # Will be set by caller
                model_used=model_used,
                processing_time=processing_time,
                response_type=response_type,
                next_action=next_action,
                conversation_state=conversation_state
            )
            
        except Exception as e:
            raise AIServiceException(f"ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    async def _process_standard_question(
        self, 
        question: str, 
        context: str, 
        file_summary: str,
        classification: QuestionClassification,
        image_data: Optional[bytes] = None,
        answer_style: Optional[str] = None
    ) -> str:
        """Process standard question without clarification needs"""
        # If question is specific enough, generate solution directly
        if self._is_specific_enough(question):
            return await self._generate_solution_with_context(
                question,  # Use the question as the task
                question,  # Use the question as understanding
                context,
                file_summary
            )
        
        # Otherwise, use classification-based processing
        if classification.classification == "simple":
            return await self.generate_simple_response(question, answer_style)
        elif classification.classification == "creative":
            return await self.generate_creative_response(question, context, answer_style)
        elif classification.classification == "analytical":
            return await self.generate_analytical_response(question, file_summary, answer_style)
        elif classification.classification == "debugging":
            return await self.generate_debugging_response(context, question, image_data)
        else:
            # Complex question - generate planning response
            return await self.generate_planning_response(context, file_summary, answer_style)
    
    async def _generate_solution_with_context(
        self,
        original_question: str,
        understanding: str,
        context: str,
        file_summary: str
    ) -> str:
        """Generate solution based on gathered context and understanding"""
        try:
            # Check if this is a new question or continuation
            is_new_question = self._is_new_question(original_question, context)
            
            if is_new_question:
                # This is a new question - treat it as a fresh request
                prompt = f"""ì‚¬ìš©ìì˜ ìƒˆë¡œìš´ ì§ˆë¬¸: "{original_question}"

ì´ì „ ëŒ€í™” ë§¥ë½:
{context}

íŒŒì¼ ì •ë³´: {file_summary}

ì´ê²ƒì€ ì´ì „ ì§ˆë¬¸ê³¼ ë‹¤ë¥¸ ìƒˆë¡œìš´ ìš”ì²­ì…ë‹ˆë‹¤. ì´ì „ ë‹µë³€ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , ìƒˆë¡œìš´ ì§ˆë¬¸ì— ëŒ€í•œ í•´ê²°ì±…ì„ ì œê³µí•´ì£¼ì„¸ìš”.

## ğŸ¯ ìƒˆë¡œìš´ í•´ê²°ì±…

[ìƒˆë¡œìš´ ë¬¸ì œ ìš”ì•½]
- ì‚¬ìš©ìê°€ ìƒˆë¡œ ìš”ì²­í•œ ë¬¸ì œ

[í•´ê²° ë°©ë²•]
- êµ¬ì²´ì ì¸ í•´ê²° ë°©ë²• (í•¨ìˆ˜, ìˆ˜ì‹, ì½”ë“œ ë“±)

[ì‚¬ìš© ë°©ë²•]
- ë‹¨ê³„ë³„ ì‚¬ìš© ë°©ë²•

[ì£¼ì˜ì‚¬í•­]
- ì£¼ì˜í•´ì•¼ í•  ì ë“¤

[ì¶”ê°€ íŒ]
- ë” ë‚˜ì€ ë°©ë²•ì´ë‚˜ ê°œì„  ë°©ì•ˆ"""
            else:
                # This is a continuation or clarification
                prompt = f"""ì´ì „ ëŒ€í™”ë¥¼ ì´ì–´ì„œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.

ì›ë˜ ì§ˆë¬¸: "{original_question}"
í˜„ì¬ ìƒí™©: {understanding}

ì´ì „ ëŒ€í™” ë‚´ìš©:
{context}

íŒŒì¼ ì •ë³´: {file_summary}

ì´ì „ ëŒ€í™”ë¥¼ ê³ ë ¤í•˜ì—¬ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

1. **ì´ì „ ë‹µë³€ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ë³´ì™„**ì´ í•„ìš”í•œ ê²½ìš°
2. **ìƒˆë¡œìš´ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­**ì´ ìˆëŠ” ê²½ìš°
3. **ì´ì „ í•´ê²°ì±…ì˜ ì‹¤í–‰ ê²°ê³¼**ì— ëŒ€í•œ í”¼ë“œë°±ì´ ìˆëŠ” ê²½ìš°

ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ë©° ì‚¬ìš©ìì˜ ìš”êµ¬ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

            # Use Gemini 2.5 Pro for VBA and complex code generation
            return await self._call_gemini(prompt, temperature=0.3)
            
        except Exception as e:
            raise AIServiceException(f"í•´ê²°ì±… ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    def _is_new_question(self, question: str, context: str) -> bool:
        """Check if the question is new or a continuation"""
        question_lower = question.lower()
        
        # Keywords that indicate a new question
        new_question_indicators = [
            "ì¶”ê°€", "ë˜", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ë‹¤ìŒ", "ì´ë²ˆì—ëŠ”", "ì´ì œ", "ìƒˆë¡œ",
            "ë‹¤ë¥¸", "ë³„ë„", "ì¶”ê°€ë¡œ", "ë¶€ê°€ë¡œ", "ê·¸ ë‹¤ìŒ", "ê·¸ë¦¬ê³  ë‚˜ì„œ",
            "eì—´", "fì—´", "gì—´", "ìƒˆ ì—´", "ë‹¤ë¥¸ ì—´", "ë‹¤ë¥¸ ì‹œíŠ¸", "ìƒˆ ì‹œíŠ¸"
        ]
        
        # Check if question contains new question indicators
        has_new_indicators = any(indicator in question_lower for indicator in new_question_indicators)
        
        # Check if question mentions different columns/sheets than previous context
        context_lower = context.lower()
        has_different_targets = self._has_different_targets(question_lower, context_lower)
        
        # Check if question is about a different function/operation
        has_different_operation = self._has_different_operation(question_lower, context_lower)
        
        return has_new_indicators or has_different_targets or has_different_operation
    
    def _has_different_targets(self, question: str, context: str) -> bool:
        """Check if question targets different columns/sheets than context"""
        # Extract column/sheet references
        import re
        
        # Find column references (Aì—´, Bì—´, etc.)
        question_cols = re.findall(r'[a-z]ì—´', question)
        context_cols = re.findall(r'[a-z]ì—´', context)
        
        # If question mentions columns not in context, it's likely new
        if question_cols and context_cols:
            return not any(col in context_cols for col in question_cols)
        
        return False
    
    def _has_different_operation(self, question: str, context: str) -> bool:
        """Check if question is about a different operation than context"""
        # Different function keywords
        functions = {
            "vlookup": ["vlookup", "vlookup"],
            "xlookup": ["xlookup", "xlookup"],
            "if": ["if", "ì¡°ê±´", "í™•ì¸"],
            "isnumber": ["isnumber", "ìˆ«ìì¸ì§€", "ìˆ«ì í™•ì¸"],
            "istext": ["istext", "ë¬¸ìì¸ì§€", "ë¬¸ì í™•ì¸"],
            "sumif": ["sumif", "ì¡°ê±´í•©ê³„"],
            "countif": ["countif", "ì¡°ê±´ê°œìˆ˜"]
        }
        
        # Find functions mentioned in question and context
        question_funcs = []
        context_funcs = []
        
        for func_name, keywords in functions.items():
            if any(keyword in question for keyword in keywords):
                question_funcs.append(func_name)
            if any(keyword in context for keyword in keywords):
                context_funcs.append(func_name)
        
        # If question mentions different functions than context, it's likely new
        if question_funcs and context_funcs:
            return not any(func in context_funcs for func in question_funcs)
        
        return False
    
    def _is_specific_task_request(self, question: str) -> bool:
        """Check if the question contains a specific task request"""
        question_lower = question.lower()
        
        # Check for specific function names
        function_keywords = [
            "vlookup", "xlookup", "hlookup", "index", "match", "sumif", "countif", 
            "averageif", "if", "and", "or", "concatenate", "left", "right", "mid",
            "len", "trim", "substitute", "replace", "find", "search", "date", "today",
            "now", "year", "month", "day", "weekday", "eomonth", "datedif",
            "isnumber", "istext", "isna", "isblank", "iserror", "ìˆ«ìì¸ì§€", "ë¬¸ìì¸ì§€", "í™•ì¸"
        ]
        
        # Check for specific operations
        operation_keywords = [
            "ì°¾ì•„ì„œ", "ê°€ì ¸ì™€", "ì—°ê²°", "í•©ê³„", "í‰ê· ", "ê°œìˆ˜", "ì •ë ¬", "í•„í„°", 
            "ì¤‘ë³µ ì œê±°", "ì •ë¦¬", "ë¶„ì„", "ìš”ì•½", "ê·¸ë˜í”„", "ì°¨íŠ¸", "ë§¤í¬ë¡œ", "ìë™í™”",
            "ì¡°ê±´ë¶€", "ì„œì‹", "ìˆ˜ì‹", "í•¨ìˆ˜", "ì½”ë“œ", "ìŠ¤í¬ë¦½íŠ¸", "vba",
            "í™•ì¸í•˜ê³ ", "í™•ì¸í•˜ê³  ì‹¶ì–´", "ì•Œê³  ì‹¶ì–´", "ì›í•´", "í•„ìš”í•´"
        ]
        
        # Check for specific Excel terms
        excel_keywords = [
            "ì—´", "í–‰", "ì…€", "ì‹œíŠ¸", "ì›Œí¬ë¶", "ë²”ìœ„", "í”¼ë²—", "í…Œì´ë¸”", "ë°ì´í„°",
            "ê°’", "ì°¸ì¡°", "ë§í¬", "ë³µì‚¬", "ë¶™ì—¬ë„£ê¸°", "ì‚½ì…", "ì‚­ì œ", "ì´ë™"
        ]
        
        # Check for column references (Aì—´, Bì—´, etc.)
        import re
        column_pattern = r'[a-z]ì—´'
        has_column_ref = bool(re.search(column_pattern, question_lower))
        
        # Check if question contains specific function or operation
        has_function = any(func in question_lower for func in function_keywords)
        has_operation = any(op in question_lower for op in operation_keywords)
        has_excel_term = any(term in question_lower for term in excel_keywords)
        
        # Consider it specific if it has:
        # 1. Function names, OR
        # 2. Column references with operations, OR
        # 3. Detailed operations with Excel terms
        return has_function or (has_column_ref and has_operation) or (has_operation and has_excel_term)
    
    def _is_vba_or_complex_request(self, question: str) -> bool:
        """Check if the question is about VBA or complex operations"""
        question_lower = question.lower()
        
        # VBA related keywords
        vba_keywords = [
            "vba", "ë§¤í¬ë¡œ", "ì½”ë“œ", "ìŠ¤í¬ë¦½íŠ¸", "ìë™í™”", "í”„ë¡œê·¸ë¨", "í•¨ìˆ˜", "ì„œë¸Œë£¨í‹´",
            "for", "while", "if", "then", "else", "end if", "loop", "next", "dim", "set"
        ]
        
        # Complex operation keywords
        complex_keywords = [
            "í†µí•©", "í•©ì¹˜ê¸°", "ë³‘í•©", "ì—°ê²°", "ëª¨ë“ ", "ì „ì²´", "ì‹œíŠ¸", "íŒŒì¼", "ë…„ë„", "ì›”ë³„",
            "ë§¤ì¶œ", "ìë£Œ", "ë°ì´í„°", "ê´€ë¦¬", "ì •ë¦¬", "ë¶„ì„", "ìš”ì•½", "ì§‘ê³„", "í†µê³„",
            "1ì›”", "2ì›”", "3ì›”", "4ì›”", "5ì›”", "6ì›”", "7ì›”", "8ì›”", "9ì›”", "10ì›”", "11ì›”", "12ì›”",
            "ê° ì´ë¦„", "ê° ì‹œíŠ¸", "ì—¬ëŸ¬ ì‹œíŠ¸", "ì—¬ëŸ¬ íŒŒì¼", "ë…„ë„ë³„", "ì›”ë³„"
        ]
        
        # Check for VBA keywords
        has_vba = any(keyword in question_lower for keyword in vba_keywords)
        
        # Check for complex operation keywords (need multiple matches for complex operations)
        complex_matches = sum(1 for keyword in complex_keywords if keyword in question_lower)
        has_complex_operation = complex_matches >= 3  # At least 3 complex keywords
        
        # Check for specific patterns that indicate complex operations
        has_specific_patterns = any([
            "ì‹œíŠ¸ì— ì €ì¥" in question_lower,
            "íŒŒì¼ë¡œ ê´€ë¦¬" in question_lower,
            "í•œ ì‹œíŠ¸ì— í†µí•©" in question_lower,
            "ëª¨ë“  ë§¤ì¶œìë£Œ" in question_lower,
            "ë…„ë„ë³„ë¡œ" in question_lower
        ])
        
        return has_vba or has_complex_operation or has_specific_patterns
    
    def _is_specific_enough(self, question: str) -> bool:
        """Check if the question is specific enough to process directly"""
        question_lower = question.lower()
        
        # Check for specific functions, operations, or targets
        specific_indicators = [
            # Functions
            "vlookup", "xlookup", "hlookup", "index", "match", "sumif", "countif",
            "averageif", "if", "and", "or", "concatenate", "left", "right", "mid",
            "len", "trim", "substitute", "replace", "find", "search", "date", "today",
            "now", "year", "month", "day", "weekday", "eomonth", "datedif",
            "isnumber", "istext", "isna", "isblank", "iserror", "ìˆ«ìì¸ì§€", "ë¬¸ìì¸ì§€", "í™•ì¸",
            
            # Operations
            "ì°¾ì•„ì„œ", "ê°€ì ¸ì™€", "ì—°ê²°", "í•©ê³„", "í‰ê· ", "ê°œìˆ˜", "ì •ë ¬", "í•„í„°",
            "ì¤‘ë³µ ì œê±°", "ì •ë¦¬", "ë¶„ì„", "ìš”ì•½", "ê·¸ë˜í”„", "ì°¨íŠ¸", "ë§¤í¬ë¡œ", "ìë™í™”",
            "ì¡°ê±´ë¶€", "ì„œì‹", "ìˆ˜ì‹", "í•¨ìˆ˜", "ì½”ë“œ", "ìŠ¤í¬ë¦½íŠ¸", "vba",
            "í™•ì¸í•˜ê³ ", "í™•ì¸í•˜ê³  ì‹¶ì–´", "ì•Œê³  ì‹¶ì–´", "ì›í•´", "í•„ìš”í•´",
            
            # Targets
            "aì—´", "bì—´", "cì—´", "dì—´", "eì—´", "fì—´", "gì—´", "hì—´", "iì—´", "jì—´",
            "1í–‰", "2í–‰", "3í–‰", "4í–‰", "5í–‰", "ì‹œíŠ¸", "íŒŒì¼", "ë°ì´í„°",
            
            # VBA and complex operations
            "vba", "ë§¤í¬ë¡œ", "ì½”ë“œ", "ìŠ¤í¬ë¦½íŠ¸", "ìë™í™”", "í”„ë¡œê·¸ë¨", "í•¨ìˆ˜", "ì„œë¸Œë£¨í‹´",
            "í†µí•©", "í•©ì¹˜ê¸°", "ë³‘í•©", "ì—°ê²°", "ëª¨ë“ ", "ì „ì²´", "ë…„ë„", "ì›”ë³„",
            "ë§¤ì¶œ", "ìë£Œ", "ê´€ë¦¬", "ì •ë¦¬", "ë¶„ì„", "ìš”ì•½", "ì§‘ê³„", "í†µê³„",
            "1ì›”", "2ì›”", "3ì›”", "4ì›”", "5ì›”", "6ì›”", "7ì›”", "8ì›”", "9ì›”", "10ì›”", "11ì›”", "12ì›”",
            "ê° ì´ë¦„", "ê° ì‹œíŠ¸", "ì—¬ëŸ¬ ì‹œíŠ¸", "ì—¬ëŸ¬ íŒŒì¼", "ë…„ë„ë³„", "ì›”ë³„"
        ]
        
        # Check if question contains specific indicators
        has_specific_indicators = any(indicator in question_lower for indicator in specific_indicators)
        
        # Check for column references (Aì—´, Bì—´, etc.)
        import re
        column_pattern = r'[a-z]ì—´'
        has_column_ref = bool(re.search(column_pattern, question_lower))
        
        # Check for specific patterns
        has_specific_patterns = any([
            "ì‹œíŠ¸ì— ì €ì¥" in question_lower,
            "íŒŒì¼ë¡œ ê´€ë¦¬" in question_lower,
            "í•œ ì‹œíŠ¸ì— í†µí•©" in question_lower,
            "ëª¨ë“  ë§¤ì¶œìë£Œ" in question_lower,
            "ë…„ë„ë³„ë¡œ" in question_lower,
            "vbaë¡œ" in question_lower,
            "ë§¤í¬ë¡œë¡œ" in question_lower
        ])
        
        return has_specific_indicators or has_column_ref or has_specific_patterns
    
    def _has_multiple_interpretations(self, question: str) -> bool:
        """Check if the question could have multiple interpretations"""
        question_lower = question.lower()
        
        # Ambiguous phrases that could mean different things
        ambiguous_phrases = [
            "ì •ë¦¬í•´ì¤˜",  # ë°ì´í„° ì •ë¦¬? ì„œì‹ ì •ë¦¬? 
            "ë¶„ì„í•´ì¤˜",  # ì–´ë–¤ ë¶„ì„?
            "ìš”ì•½í•´ì¤˜",  # ì–´ë–¤ ìš”ì•½?
            "ë§Œë“¤ì–´ì¤˜",  # ë¬´ì—‡ì„ ë§Œë“¤ê¹Œ?
            "í•´ì¤˜",      # ë„ˆë¬´ ì¼ë°˜ì 
            "ë„ì™€ì¤˜"     # ë„ˆë¬´ ì¼ë°˜ì 
        ]
        
        # Check if question contains only ambiguous phrases without specific context
        has_ambiguous = any(phrase in question_lower for phrase in ambiguous_phrases)
        
        # If it has ambiguous phrases but no specific context, it's unclear
        if has_ambiguous and not self._is_specific_enough(question):
            return True
            
        return False
    
    def _get_task_examples(self, task_type: str) -> str:
        """Get examples for specific task type"""
        examples = {
            "ìˆ˜ì‹/í•¨ìˆ˜ ë§Œë“¤ê¸°": """â€¢ VLOOKUPìœ¼ë¡œ ë‹¤ë¥¸ ì‹œíŠ¸ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
â€¢ SUMIFë¡œ ì¡°ê±´ì— ë§ëŠ” ê°’ í•©ê³„
â€¢ INDEX/MATCHë¡œ ìœ ì—°í•œ ë°ì´í„° ê²€ìƒ‰
â€¢ IF í•¨ìˆ˜ë¡œ ì¡°ê±´ë¶€ ê³„ì‚°
â€¢ COUNTIFë¡œ ì¡°ê±´ì— ë§ëŠ” ê°œìˆ˜ ì„¸ê¸°""",
            
            "ë°ì´í„° ì •ë¦¬": """â€¢ ì¤‘ë³µ ë°ì´í„° ì œê±°
â€¢ íŠ¹ì • ì—´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
â€¢ ë¹ˆ ì…€ ì •ë¦¬ ë° ì±„ìš°ê¸°
â€¢ ë°ì´í„° í˜•ì‹ í†µì¼
â€¢ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°""",
            
            "ìš”ì•½/ë¶„ì„": """â€¢ í”¼ë²— í…Œì´ë¸”ë¡œ ë°ì´í„° ìš”ì•½
â€¢ ë§¤ì¶œ/ì§€ì¶œ í†µê³„ ë¶„ì„
â€¢ ì›”ë³„/ë¶„ê¸°ë³„ ì§‘ê³„
â€¢ í‰ê· , ìµœëŒ€, ìµœì†Œê°’ ê³„ì‚°
â€¢ ë°ì´í„° íŒ¨í„´ ë¶„ì„""",
            
            "ì‹œê°í™”": """â€¢ ë§¤ì¶œ ì¶”ì´ ì„ ê·¸ë˜í”„
â€¢ ì¹´í…Œê³ ë¦¬ë³„ ë§‰ëŒ€ê·¸ë˜í”„
â€¢ ë¹„ìœ¨ ë¶„ì„ ì›í˜•ì°¨íŠ¸
â€¢ ë°ì´í„° ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
â€¢ ìƒê´€ê´€ê³„ ì‚°ì ë„""",
            
            "ìë™í™”": """â€¢ ë°˜ë³µ ì‘ì—… VBA ë§¤í¬ë¡œ
â€¢ ìë™ ë°ì´í„° ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â€¢ ì¡°ê±´ë¶€ ì„œì‹ ìë™ ì ìš©
â€¢ ì´ë©”ì¼ ìë™ ë°œì†¡
â€¢ íŒŒì¼ ìë™ ì €ì¥ ë° ë°±ì—…"""
        }
        return examples.get(task_type, "â€¢ êµ¬ì²´ì ì¸ ì‘ì—…ì„ ë§ì”€í•´ ì£¼ì„¸ìš”")
    
    def get_service_status(self) -> Dict[str, Any]:
        """Get comprehensive AI service status with optimized model information"""
        return {
            "openai_available": self.openai_client is not None,
            "gemini_available": self.gemini_model is not None,
            "gemini_flash_available": self.gemini_flash_model is not None,
            "models": {
                "openai": settings.OPENAI_MODEL,
                "gemini": settings.GEMINI_PRO_MODEL,
                "gemini_flash": settings.GEMINI_FLASH_MODEL
            },
            "capabilities": {
                "text_generation": True,
                "image_analysis": self.gemini_flash_model is not None,
                "fast_processing": self.gemini_flash_model is not None,
                "creative_thinking": self.gemini_model is not None,
                "code_generation": self.openai_client is not None,
                "data_analysis": self.gemini_model is not None,
                "planning": self.gemini_model is not None
            },
            "optimization": {
                "classification": "Gemini 2.0 Flash (fastest)",
                "simple_questions": "Gemini 2.0 Flash (cost-effective)",
                "creative_tasks": "Gemini 2.0 Pro (innovative)",
                "analytical_tasks": "Gemini 2.0 Pro (pattern recognition)",
                "coding_tasks": "OpenAI GPT-4o-mini (best code generation)",
                "debugging": "OpenAI GPT-4o-mini (problem-solving)",
                "image_analysis": "Gemini 2.0 Flash (multimodal)",
                "planning": "Gemini 2.0 Pro (structured thinking)"
            }
        }

# Global AI service instance
ai_service = AIService()
