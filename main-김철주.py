# --- 1. í•„ìš”í•œ ë„êµ¬ë“¤ ê°€ì ¸ì˜¤ê¸° ---
from fastapi import FastAPI, File, UploadFile, Form, HTTPException, Request
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
# from dotenv import load_dotenv  # ë°°í¬ í™˜ê²½ì—ì„œëŠ” ì´ ì¤„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
import os
import shutil
import pandas as pd
from openai import OpenAI
import google.generativeai as genai
from PIL import Image
import io
import uuid

# --- 2. ì´ˆê¸° ì„¤ì • ---
# load_dotenv()  # ë°°í¬ í™˜ê²½ì—ì„œëŠ” ì´ ì¤„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
app = FastAPI()

app.mount("/static", StaticFiles(directory="static"), name="static")

# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… ë””ë²„ê¹… ì½”ë“œ ì‹œì‘ â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
# Render ì„œë²„ê°€ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì œëŒ€ë¡œ ì½ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ê°’ì„ ë³€ìˆ˜ì— ì €ì¥í•©ë‹ˆë‹¤.
gemini_api_key_from_env = os.getenv("GEMINI_API_KEY")
openai_api_key_from_env = os.getenv("OPENAI_API_KEY")

# ì„œë²„ ë¡œê·¸ì— ìƒíƒœë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ì´ ë¡œê·¸ë¥¼ Renderì—ì„œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
print("--- STARTING SERVER & CHECKING ENVIRONMENT VARIABLES ---")
if gemini_api_key_from_env:
    # í‚¤ ì „ì²´ë¥¼ ë¡œê·¸ì— ë…¸ì¶œí•˜ì§€ ì•Šë„ë¡ ì• 5ìë¦¬ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
    print(f"[SUCCESS] Gemini API Key loaded. Starts with: {gemini_api_key_from_env[:5]}")
else:
    print("[ERROR] FAILED to load Gemini API Key. Value is None.")

if openai_api_key_from_env:
    print(f"[SUCCESS] OpenAI API Key loaded. Starts with: {openai_api_key_from_env[:5]}")
else:
    print("[ERROR] FAILED to load OpenAI API Key. Value is None.")
print("---------------------------------------------------------")
# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… ë””ë²„ê¹… ì½”ë“œ ë â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…

# OpenAI: ì •ë°€í•œ ì½”ë“œ/ìˆ˜ì‹/ë…¼ë¦¬ ë‹´ë‹¹
# ì´ì œ os.getenv() ëŒ€ì‹  ìœ„ì—ì„œ í™•ì¸í•œ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
openai_client = OpenAI(api_key=openai_api_key_from_env)
OPENAI_MODEL = "gpt-4-turbo"

# Google Gemini: ì—­í• ì— ë”°ë¼ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
# ì´ì œ os.getenv() ëŒ€ì‹  ìœ„ì—ì„œ í™•ì¸í•œ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
genai.configure(api_key=gemini_api_key_from_env)
gemini_pro_model = genai.GenerativeModel('gemini-1.5-pro-latest') 
gemini_flash_model = genai.GenerativeModel('gemini-1.5-flash-latest')

templates = Jinja2Templates(directory="templates")
chat_sessions = {}

# --- 3. AI í˜ë¥´ì†Œë‚˜ ë° í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
# (ì½”ë“œ ê¸¸ì´ìƒ ìƒëµ) ì‹¤ì œ íŒŒì¼ì—ëŠ” PERSONA_PROMPTì™€ DEBUGGING_PERSONA_PROMPT ë‚´ìš©ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
PERSONA_PROMPT = """..."""
DEBUGGING_PERSONA_PROMPT = """..."""

# --- 4. ì›¹í˜ì´ì§€ ë³´ì—¬ì£¼ê¸° ---
@app.get("/", response_class=HTMLResponse)
async def serve_home_page(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

# --- 5. AIì—ê²Œ ì§ˆë¬¸/í”¼ë“œë°±í•˜ëŠ” í•µì‹¬ ê¸°ëŠ¥ ---
@app.post("/ask")
async def handle_ask_request(
    question: str = Form(""),
    file: UploadFile = File(None),
    image: UploadFile = File(None),
    session_id: str = Form(None),
    is_feedback: bool = Form(False)
):
    # (ì´í•˜ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼í•˜ë¯€ë¡œ ìƒëµí•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤)
    # 5.1. ì„¸ì…˜ ê´€ë¦¬
    if not session_id or session_id not in chat_sessions:
        session_id = str(uuid.uuid4())
        chat_sessions[session_id] = []

    # 5.2. ì…ë ¥ ë°ì´í„° ë¶„ì„ ë° í…ìŠ¤íŠ¸í™”
    final_input_text = f"ì‚¬ìš©ì ì§ˆë¬¸/í”¼ë“œë°±: {question}\n\n"
    
    if file and file.filename:
        pass 

    if image and image.filename:
        try:
            image_content = await image.read()
            img = Image.open(io.BytesIO(image_content))
            buffered = io.BytesIO()
            img.save(buffered, format="PNG")
            img_bytes = buffered.getvalue()
            
            response = gemini_flash_model.generate_content(
                ["ì´ ì´ë¯¸ì§€ì— ìˆëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ì™€ ìƒí™©ì„ ì„¤ëª…í•´ì¤˜.", {"mime_type": "image/png", "data": img_bytes}]
            )
            image_description = response.text
            final_input_text += f"[ì‚¬ìš©ì ì²¨ë¶€ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ğŸ“¸]\n{image_description}\n------------------\n"
        except Exception as e:
            print(f"ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ë³€í™˜ ì˜¤ë¥˜: {e}")
            final_input_text += "[ì‚¬ìš©ì ì²¨ë¶€ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ]\n"
    
    ai_answer = ""
    try:
        if is_feedback:
            previous_answer = next((msg['content'] for msg in reversed(chat_sessions.get(session_id, [])) if msg['role'] == 'assistant'), "ì´ì „ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            debugging_prompt = f"{DEBUGGING_PERSONA_PROMPT}\n\n--- ì´ì „ í•´ê²°ì±… ---\n{previous_answer}\n\n--- ì‚¬ìš©ì í”¼ë“œë°± ---\n{final_input_text}"
            response = openai_client.chat.completions.create(
                model=OPENAI_MODEL, messages=[{"role": "user", "content": debugging_prompt}]
            )
            ai_answer = response.choices[0].message.content
        else:
            is_code_request = any(k in question.lower() for k in ["vba", "ë§¤í¬ë¡œ", "ìŠ¤í¬ë¦½íŠ¸", "ì½”ë“œ"])
            is_complex_request = any(k in question.lower() for k in ["ë³µì¡í•œ", "ë°°ì—´ ìˆ˜ì‹", "ìˆ˜ì‹ ì¡°í•©", "ë¶„ì„"])
            is_file_attached = file is not None

            if is_file_attached and (is_code_request or is_complex_request):
                context_prompt = f"ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì—…ë¡œë“œí•œ íŒŒì¼ì˜ ì •ë³´ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì½”ë“œë¥¼ ì‘ì„±í•˜ê±°ë‚˜ ë³µì¡í•œ ë¶„ì„ì„ í•˜ê¸° ìœ„í•´ í•„ìš”í•œ í•µì‹¬ ì •ë³´(ì‹œíŠ¸, ì»¬ëŸ¼, ë°ì´í„° íŠ¹ì§• ë“±)ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n{final_input_text}"
                context_response = gemini_pro_model.generate_content(context_prompt)
                extracted_context = context_response.text
                final_prompt_for_openai = f"{PERSONA_PROMPT}\n\n--- AI ìš”ì•½ ì •ë³´ ---\n{extracted_context}\n\n--- ì›ë³¸ ì§ˆë¬¸ ---\n{question}\n\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœê³ ì˜ ë‹µë³€ì„ ìƒì„±í•´ì¤˜."
                response = openai_client.chat.completions.create(
                    model=OPENAI_MODEL, messages=[{"role": "user", "content": final_prompt_for_openai}]
                )
                ai_answer = response.choices[0].message.content + "\n\n---\n*ğŸ’¡ Gemini Proì˜ ë¶„ì„ê³¼ GPT-4ì˜ ì •ë°€í•¨ìœ¼ë¡œ ë‹µë³€ì„ ë§Œë“¤ì—ˆì–´ìš”!*"
            elif is_code_request or is_complex_request:
                final_prompt = f"{PERSONA_PROMPT}\n\n{final_input_text}"
                response = openai_client.chat.completions.create(
                    model=OPENAI_MODEL, messages=[{"role": "user", "content": final_prompt}]
                )
                ai_answer = response.choices[0].message.content
            else:
                final_prompt = f"{PERSONA_PROMPT}\n\n{final_input_text}"
                response = gemini_pro_model.generate_content(final_prompt)
                ai_answer = response.text

        chat_sessions.setdefault(session_id, []).append({"role": "assistant", "content": ai_answer})
        return {"answer": ai_answer, "session_id": session_id}

    except Exception as e:
        # ì—ëŸ¬ ë¡œê·¸ë¥¼ ë” ìì„¸í•˜ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.
        print(f"!!! API í˜¸ì¶œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__} - {e}")
        raise HTTPException(status_code=500, detail="ìœ¼ì•…! AI ì „ë¬¸ê°€ì™€ ì—°ê²°ì´ ì ì‹œ ëŠê²¼ì–´ìš”. ğŸ˜­")